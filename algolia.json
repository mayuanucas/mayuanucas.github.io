[{"categories":["效率工具"],"content":"2021年1月1日，告别使用了三年的Hexo，正式迁移到Hugo. 原因 使用Hexo+Next+GitHub Pages搭建博客已经有三年的时间了，但是随着文章数量的增长，Hexo生成博客的速度也慢下来了，而且Hexo对Latex公式的支持不给力。怀着对Golang 的信仰，至此加入Golang的生态圈，拥抱Hugo。 开始迁移 ","date":"2021-01-01","objectID":"https://mayuanucas.github.io/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB/:0:0","tags":["博客","Hugo"],"title":"博客迁移","uri":"https://mayuanucas.github.io/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB/"},{"categories":["效率工具"],"content":"简介 Hugo is one of the most popular open-source static site generators. With its amazing speed and flexibility, Hugo makes building websites fun again. Hugo是一个基于Go语言开发的静态网站生成器，主打简单、易用、高效、易扩展、快速部署，丰富的主题也使得Hugo在个人博客站点搭建方面也使用广泛。迁移到Hugo后，安装、构建、部署整个流程相比Hexo，速度提升飞快🚀。 ","date":"2021-01-01","objectID":"https://mayuanucas.github.io/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB/:1:0","tags":["博客","Hugo"],"title":"博客迁移","uri":"https://mayuanucas.github.io/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB/"},{"categories":["效率工具"],"content":"安装 本地macOS平台直接使用Homebrew安装 brew install hugo ","date":"2021-01-01","objectID":"https://mayuanucas.github.io/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB/:2:0","tags":["博客","Hugo"],"title":"博客迁移","uri":"https://mayuanucas.github.io/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB/"},{"categories":["效率工具"],"content":"创建新站点 hugo new site blog cd blog hugo new posts/博客迁移.md 这个时候就已经创建了新的博客站点blog，并且创建了第一篇文章博客迁移.md，新建的文章位于/blog/content/posts目录下。 新建文章注意 默认情况下，所有文章新建都为草稿，草稿文章是不渲染的，需要修改头部draft: true为draft: false ","date":"2021-01-01","objectID":"https://mayuanucas.github.io/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB/:3:0","tags":["博客","Hugo"],"title":"博客迁移","uri":"https://mayuanucas.github.io/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB/"},{"categories":["效率工具"],"content":"使用主题 Hugo提供了丰富的主题，可以在这选择喜欢的主题，并添加到刚刚新加的博客站点，以我选择的LoveIt主题为例 首先将主题添加到项目blog/themes目录，根目录下执行： git clone -b master https://github.com/dillonzq/LoveIt.git themes/LoveIt 然后在/blog/config.toml配置主题参数： baseURL = \"http://example.org/\" # [en, zh-cn, fr, ...] 设置默认的语言 defaultContentLanguage = \"zh-cn\" # 网站语言, 仅在这里 CN 大写 languageCode = \"zh-CN\" # 是否包括中日韩文字 hasCJKLanguage = true # 网站标题 title = \"我的 Hugo 博客站点\" # 更改使用 Hugo 构建网站时使用的默认主题 theme = \"LoveIt\" [params] # LoveIt 主题版本 version = \"0.2.X\" [menu] [[menu.main]] identifier = \"posts\" # 你可以在名称 (允许 HTML 格式) 之前添加其他信息, 例如图标 pre = \"\" name = \"文章\" url = \"/posts/\" # 当你将鼠标悬停在此菜单链接上时, 将显示的标题 title = \"\" weight = 1 [[menu.main]] identifier = \"tags\" pre = \"\" name = \"标签\" url = \"/tags/\" title = \"\" weight = 2 [[menu.main]] identifier = \"categories\" pre = \"\" name = \"分类\" url = \"/categories/\" title = \"\" weight = 3 这个主题功能很强大，更多详细配置及功能可以参考项目Docs ","date":"2021-01-01","objectID":"https://mayuanucas.github.io/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB/:4:0","tags":["博客","Hugo"],"title":"博客迁移","uri":"https://mayuanucas.github.io/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB/"},{"categories":["效率工具"],"content":"本地展示 此时执行以下命令即可在本地 http://localhost:1313/ 预览当前站点状态 hugo serve ","date":"2021-01-01","objectID":"https://mayuanucas.github.io/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB/:5:0","tags":["博客","Hugo"],"title":"博客迁移","uri":"https://mayuanucas.github.io/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB/"},{"categories":["效率工具"],"content":"快速部署 准备好部署网站时，运行 hugo 可以快速构建网站，项目根目录下会生成public目录，其中包含博客站点所有内容和资源，直接部署在web服务器即可。 以部署到github pages为例，参考Hugo官网说明，创建public子模块，关联原先github page仓库用户名.github.io，将每次构建结果提交到远程仓库，可以通过自动部署脚本实现快速部署 #!/bin/sh # If a command fails then the deploy stops set -e printf \"\\033[0;32mDeploying updates to GitHub...\\033[0m\\n\" # Build the project. hugo -t LoveIt # if using a theme, replace with `hugo -t \u003cYOURTHEME\u003e` # Go To Public folder cd public # Add changes to git. git add . # Commit changes. msg=\"rebuilding site $(date)\" if [ -n \"$*\" ]; then msg=\"$*\" fi git commit -m \"$msg\" # Push source and build repos. git push origin master cd .. 到这一步，每次更新文章之后，需要在本地 blog 目录下手动执行 ./deploy.sh 来部署到github page、coding page等静态页面。 ","date":"2021-01-01","objectID":"https://mayuanucas.github.io/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB/:6:0","tags":["博客","Hugo"],"title":"博客迁移","uri":"https://mayuanucas.github.io/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB/"},{"categories":["效率工具"],"content":"启用搜索 基于 Lunr.js 或 algolia, LoveIt 主题支持搜索功能. ","date":"2021-01-01","objectID":"https://mayuanucas.github.io/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB/:7:0","tags":["博客","Hugo"],"title":"博客迁移","uri":"https://mayuanucas.github.io/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB/"},{"categories":["效率工具"],"content":"输出配置 为了生成搜索功能所需要的 index.json, 请在你的 网站配置 中添加 JSON 输出文件类型到 outputs 部分的 home 字段中. [outputs] home = [\"HTML\", \"RSS\", \"JSON\"] ","date":"2021-01-01","objectID":"https://mayuanucas.github.io/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB/:7:1","tags":["博客","Hugo"],"title":"博客迁移","uri":"https://mayuanucas.github.io/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB/"},{"categories":["效率工具"],"content":"搜索配置 基于 Hugo 生成的 index.json 文件, 你可以激活搜索功能. 这是你的 网站配置 中的搜索部分: [params.search] enable = true # 搜索引擎的类型 (\"lunr\", \"algolia\") type = \"algolia\" # 文章内容最长索引长度 contentLength = 4000 # 搜索框的占位提示语 placeholder = \"\" # 最大结果数目 maxResultLength = 10 # 结果内容片段长度 snippetLength = 50 # 搜索结果中高亮部分的 HTML 标签 highlightTag = \"em\" # 是否在搜索索引中使用基于 baseURL 的绝对路径 absoluteURL = true [params.search.algolia] index = \"\" appID = \"\" searchKey = \"\" 怎样选择搜索引擎? 以下是两种搜索引擎的对比: lunr: 简单, 无需同步 index.json, 没有 contentLength 的限制, 但占用带宽大且性能低 (特别是中文需要一个较大的分词依赖库) algolia: 高性能并且占用带宽低, 但需要同步 index.json 且有 contentLength 的限制 文章内容被 h2 和 h3 HTML 标签切分来提高查询效果并且基本实现全文搜索. contentLength 用来限制 h2 和 h3 HTML 标签开头的内容部分的最大长度. 关于 algolia 的使用技巧 你需要上传 index.json 到 algolia 来激活搜索功能. 你可以使用浏览器来上传 index.json 文件但是一个自动化的脚本可能效果更好. Algolia Atomic 是一个不错的选择. 为了兼容 Hugo 的多语言模式, 你需要上传不同语言的 index.json 文件到对应的 algolia index, 例如 zh-cn/index.json 或 fr/index.json… ","date":"2021-01-01","objectID":"https://mayuanucas.github.io/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB/:7:2","tags":["博客","Hugo"],"title":"博客迁移","uri":"https://mayuanucas.github.io/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB/"},{"categories":["效率工具"],"content":"自动上传 每次写完博文都手动上传索引文件无疑是痛苦的、无意义的重复劳动。 因此我们需要把上传索引文件的操作自动化，在自动部署的时候顺便完成即可。 这里我们采用npm包 atomic-algolia 来完成上传操作。 安装 atomic-algolia 包 npm init -y // npm默认生成package.json文件 npm install -g atomic-algolia // npm全局安装atomic-algolia 修改目录下的 package.json 文件 \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" \u0026\u0026 exit 1\", \"algolia\": \"atomic-algolia\" }, 注意 \"test\" 那一行末尾有个英文逗号，不要漏了。 blog 根目录下新建 .env 文件 ALGOLIA_APP_ID=你的Application ID ALGOLIA_INDEX_NAME=你的索引名字 ALGOLIA_INDEX_FILE=public/algolia.json ALGOLIA_ADMIN_KEY=你的Admin API Key 另外特别注意 ALGOLIA_ADMIN_KEY 可以用来管理你的索引，所以尽量不要提交到公共仓库。 上传索引的命令 npm run algolia // 在blog根目录下执行 后续就是把下面的命令加到你的部署脚本即可： #!/bin/sh # If a command fails then the deploy stops set -e printf \"\\033[0;32mDeploying updates to GitHub...\\033[0m\\n\" # Build the project. hugo -t LoveIt # if using a theme, replace with `hugo -t \u003cYOURTHEME\u003e` # Go To Public folder cd public # Add changes to git. git add . # Commit changes. msg=\"rebuilding site $(date)\" if [ -n \"$*\" ]; then msg=\"$*\" fi git commit -m \"$msg\" # Push source and build repos. git push origin master cd .. # 自动更新文章索引 npm run algolia 拥抱Hugo 还有更多的功能等待探索中… 目前使用下来，Hugo整体的使用体验很不错，后面会将个人文章陆续迁移到这，慢慢完善。 ","date":"2021-01-01","objectID":"https://mayuanucas.github.io/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB/:7:3","tags":["博客","Hugo"],"title":"博客迁移","uri":"https://mayuanucas.github.io/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB/"},{"categories":["效率工具"],"content":"正则表达式，英文是 Regular Expression，简称 RE。顾名思义，正则其实就是一种描述文本内容组成规律的表示方式。在编程语言中，正则常常用来简化文本处理的逻辑。在 Linux 命令中，它也可以帮助我们轻松地查找或编辑文件的内容，甚至实现整个文件夹中所有文件的内容替换，比如 grep、egrep、sed、awk、vim 等。另外，在各种文本编辑器中，比如 Atom，Sublime Text 或 VS Code 等，在查找或替换的时候也会使用到它。总之，正则是无处不在的，已经渗透到了日常工作的方方面面。 前言 简单来说，正则是一个非常强大的文本处理工具，它的应用极其广泛。我们可以利用它来校验数据的有效性，比如用户输入的手机号是不是符合规则；也可以从文本中提取想要的内容，比如从网页中抽取数据；还可以用来做文本内容替换，从而得到我们想要的内容。通过它的功能和分布的广泛你也能看出来，正则是一个非常值得花时间和精力好好学习的基本技能。之前需要花几十分钟才能搞定的事情，可能用正则很快就搞定了；之前不能解决的问题，通过系统地学习正则后，可能发现也能轻松解决了。还在犹豫什么？赶紧继续阅读吧！ 元字符 元字符就是指那些在正则表达式中具有特殊意义的专用字符，元字符是构成正则表达式的基本元件，正则就是由一系列的元字符组成的。 ","date":"2020-07-04","objectID":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/:0:0","tags":["正则表达式"],"title":"正则表达式入门","uri":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/"},{"categories":["效率工具"],"content":"元字符的分类 元字符大致分成这几类：表示单个特殊字符的，表示空白符的，表示某个范围的，表示次数的量词，另外还有表示断言的，我们可以把它理解成边界限定。 ","date":"2020-07-04","objectID":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/:1:0","tags":["正则表达式"],"title":"正则表达式入门","uri":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/"},{"categories":["效率工具"],"content":"特殊单字符 . 任意字符(换行除外) \\d 任意数字 \\D 任意非数字 \\w 任意字母、数字、下划线 \\W 任意非字母、数字、下划线 \\s 任意空白符 \\S 任意非空白符 ","date":"2020-07-04","objectID":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/:1:1","tags":["正则表达式"],"title":"正则表达式入门","uri":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/"},{"categories":["效率工具"],"content":"空白符 除了特殊单字符外，你在处理文本的时候肯定还会遇到空格、换行等空白符。其实在写代码的时候也会经常用到，换行符 \\n，TAB 制表符 \\t 等。 有编程经验的程序员肯定都知道，不同的系统在每行文本结束位置默认的“换行”会有区别。比如在 Windows 里是 \\r\\n，在 Linux 和 MacOS 中是 \\n。在正则中，也是类似于 \\n 或 \\r 等方式来表示空白符号，只要记住它们就行了。平时使用正则，大部分场景使用 \\s 就可以满足需求，\\s 代表任意单个空白符号。 \\s 能匹配上各种空白符号，也可以匹配上空格。换行有专门的表示方式，在正则中，空格就是用普通的字符英文的空格来表示。 ","date":"2020-07-04","objectID":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/:1:2","tags":["正则表达式"],"title":"正则表达式入门","uri":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/"},{"categories":["效率工具"],"content":"量词 在正则中，英文的星号 (*) 代表出现 0 到多次，加号 (+) 代表 1 到多次，问号 (?) 代表 0 到 1 次，{m,n}代表 m 到 n 次。 ","date":"2020-07-04","objectID":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/:1:3","tags":["正则表达式"],"title":"正则表达式入门","uri":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/"},{"categories":["效率工具"],"content":"范围 在正则表达式中，表示范围的符号有四个分类： 首先是管道符号，用它来隔开多个正则，表示满足其中任意一个就行，比如 ab|bc 能匹配上 ab，也能匹配上 bc，在正则有多种情况时，这个非常有用。中括号[]代表多选一，可以表示里面的任意单个字符，所以任意元音字母可以用 [aeiou] 来表示。另外，中括号中，我们还可以用中划线表示范围，比如 [a-z] 可以表示所有小写字母。如果中括号第一个是脱字符（^），那么就表示非，表达的是不能是里面的任何单个元素。 量词与贪婪 ","date":"2020-07-04","objectID":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/:1:4","tags":["正则表达式"],"title":"正则表达式入门","uri":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/"},{"categories":["效率工具"],"content":"贪婪匹配 在正则中，表示次数的量词默认是贪婪的，在贪婪模式下，会尝试尽可能最大长度去匹配。贪婪模式的特点就是尽可能进行最大长度匹配。 例如：在字符串 aaabb 中使用正则 a* 的匹配过程。a* 在匹配开头的 a 时，会尝试尽量匹配更多的 a，直到第一个字母 b 不满足要求为止，匹配上三个 a，后面每次匹配时都得到了空字符串。 ","date":"2020-07-04","objectID":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/:2:0","tags":["正则表达式"],"title":"正则表达式入门","uri":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/"},{"categories":["效率工具"],"content":"非贪婪匹配 非贪婪模式会尽可能短地去匹配，在量词后面加上英文的问号 (?)，正则就变成了 a*?。 ","date":"2020-07-04","objectID":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/:3:0","tags":["正则表达式"],"title":"正则表达式入门","uri":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/"},{"categories":["效率工具"],"content":"独占模式 独占模式和贪婪模式很像，独占模式会尽可能多地去匹配，如果匹配失败就结束，不会进行回溯，这样的话就比较节省时间。具体的方法就是在量词后面加上加号（+）。 独占模式性能比较好，可以节约匹配的时间和 CPU 资源，但有些情况下并不能满足需求，要想使用这个模式还要看具体需求（比如我们接下来要讲的案例），另外还得看你当前使用的语言或库的支持程度。 分组与引用 ","date":"2020-07-04","objectID":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/:4:0","tags":["正则表达式"],"title":"正则表达式入门","uri":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/"},{"categories":["效率工具"],"content":"分组与编号 括号在正则中可以用于分组，被括号括起来的部分“子表达式”会被保存成一个子组。分组和编号的规则，用一句话来说就是，第几个括号就是第几个分组。以时间格式 2020-05-10 20:23:05为例，假设想要使用正则提取出里面的日期和时间。 可以写出如图所示的正则，将日期和时间都括号括起来。这个正则中一共有两个分组，日期是第 1 个，时间是第 2 个。 ","date":"2020-07-04","objectID":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/:5:0","tags":["正则表达式"],"title":"正则表达式入门","uri":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/"},{"categories":["效率工具"],"content":"不保存子组 在括号里面的会保存成子组，但有些情况下，你可能只想用括号将某些部分看成一个整体，后续不用再用它，类似这种情况，在实际使用时，是没必要保存子组的。这时可以在括号里面使用 ?: 不保存子组。由于子组变少了，正则性能会更好，在子组计数时也更不容易出错。那什么是不保存子组呢？可以理解成，括号只用于归组，把某个部分当成“单个元素”，不分配编号，后面不会再进行这部分的引用。 ","date":"2020-07-04","objectID":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/:6:0","tags":["正则表达式"],"title":"正则表达式入门","uri":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/"},{"categories":["效率工具"],"content":"括号嵌套 要看某个括号里面的内容是第几个分组怎么办？不要担心，其实方法很简单，我们只需要数左括号（开括号）是第几个，就可以确定是第几个子组。 ","date":"2020-07-04","objectID":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/:7:0","tags":["正则表达式"],"title":"正则表达式入门","uri":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/"},{"categories":["效率工具"],"content":"命名分组 命名分组的格式为(?P\u003c分组名\u003e正则)，例如 Django的路由中，命名分组示例如下： url(r'^profile/(?P\u003cusername\u003e\\w+)/$', view_func) ","date":"2020-07-04","objectID":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/:8:0","tags":["正则表达式"],"title":"正则表达式入门","uri":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/"},{"categories":["效率工具"],"content":"分组引用 大部分情况下，我们就可以使用 “反斜扛 + 编号”，即 \\number 的方式来进行引用，而 JavaScript 中是通过“$”编号来引用，如“$1”。 匹配模式 ","date":"2020-07-04","objectID":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/:9:0","tags":["正则表达式"],"title":"正则表达式入门","uri":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/"},{"categories":["效率工具"],"content":"不区分大小写模式 把模式修饰符放在整个正则前面时，就表示整个正则表达式都是不区分大小写的。模式修饰符是通过 (? 模式标识) 的方式来表示的。 我们只需要把模式修饰符放在对应的正则前，就可以使用指定的模式了。在不区分大小写模式中，由于不分大小写的英文是 Case-Insensitive，那么对应的模式标识就是 I 的小写字母 i，所以不区分大小写的 cat 就可以写成 (?i)cat。 也可以用它来尝试匹配两个连续出现的 cat，如下图所示，你会发现，即便是第一个 cat 和第二个 cat 大小写不一致，也可以匹配上。 如果我们想要前面匹配上的结果，和第二次重复时的大小写一致，那该怎么做呢？我们只需要用括号把修饰符和正则 cat 部分括起来，加括号相当于作用范围的限定，让不区分大小写只作用于这个括号里的内容。 通过修饰符指定匹配模式的方式，在大部分编程语言中都是可以直接使用的，但在 JS 中我们需要使用 /regex/i 来指定匹配模式。在编程语言中通常会提供一些预定义的常量，来进行匹配模式的指定。比如 Python 中可以使用 re.IGNORECASE 或 re.I ，来传入正则函数中来表示不区分大小写。 import re re.findall(r\"cat\", \"CAT Cat cat\", re.IGNORECASE) # 输出结果为： ['CAT', 'Cat', 'cat'] 总结一下不区分大小写模式的要点： 不区分大小写模式的指定方式，使用模式修饰符 (?i)； 修饰符如果在括号内，作用范围是这个括号内的正则，而不是整个正则； 使用编程语言时可以使用预定义好的常量来指定匹配模式。 ","date":"2020-07-04","objectID":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/:10:0","tags":["正则表达式"],"title":"正则表达式入门","uri":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/"},{"categories":["效率工具"],"content":"点号通配模式（Dot All） 英文的点（.）可以匹配上任何符号，但不能匹配换行。当我们需要匹配真正的“任意”符号的时候，可以使用 [\\s\\S] 或 [\\d\\D] 或 [\\w\\W] 等。 但是这么写不够简洁自然，所以正则中提供了一种模式，让英文的点（.）可以匹配上包括换行的任何字符。 这个模式就是点号通配模式，有很多地方把它称作单行匹配模式，但这么说容易造成误解，毕竟它与多行匹配模式没有联系，因此在这里统一用更容易理解的“点号通配模式”。单行的英文表示是 Single Line，单行模式对应的修饰符是 (?s)，这里选择用 the cat 来举一个点号通配模式的例子。如下图所示： ","date":"2020-07-04","objectID":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/:11:0","tags":["正则表达式"],"title":"正则表达式入门","uri":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/"},{"categories":["效率工具"],"content":"多行匹配模式（Multiline） 通常情况下，^ 匹配整个字符串的开头，$ 匹配整个字符串的结尾。多行匹配模式改变的就是 \\ ^ 和 $ 的匹配行为。 多行模式的作用在于，使 ^ 和 $ 能匹配上每行的开头或结尾，我们可以使用模式修饰符号 (?m) 来指定这个模式。 ","date":"2020-07-04","objectID":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/:12:0","tags":["正则表达式"],"title":"正则表达式入门","uri":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/"},{"categories":["效率工具"],"content":"注释模式（Comment） 在写代码的时候，通常会在一些关键的地方加上注释，让代码更易于理解。很多语言也支持在正则中添加注释，让正则更容易阅读和维护，这就是正则的注释模式。正则中注释模式是使用 (?#comment) 来表示。例如： (\\w+)(?#word) \\1(?#word repeat again) 断言 ","date":"2020-07-04","objectID":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/:13:0","tags":["正则表达式"],"title":"正则表达式入门","uri":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/"},{"categories":["效率工具"],"content":"单词边界 在正则中使用\\b 来表示单词的边界。 \\b 中的 b 可以理解为是边界（Boundary）这个单词的首字母。 ","date":"2020-07-04","objectID":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/:14:0","tags":["正则表达式"],"title":"正则表达式入门","uri":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/"},{"categories":["效率工具"],"content":"行的开始或结束 和单词的边界类似，在正则中还有文本每行的开始和结束，如果我们要求匹配的内容要出现在一行文本开头或结尾，就可以使用 ^ 和 $ 来进行位置界定。 ","date":"2020-07-04","objectID":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/:15:0","tags":["正则表达式"],"title":"正则表达式入门","uri":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/"},{"categories":["效率工具"],"content":"环视 左尖括号代表看左边，没有尖括号是看右边，感叹号是非的意思。 因此，针对邮编的问题，就可以写成左边不是数字，右边也不是数字的 6 位数的正则。即 (?\u003c!\\d)[1-9]\\d{5}(?!\\d) 例如：用正则分组引用来实现替换重复出现的单词。 ","date":"2020-07-04","objectID":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/:16:0","tags":["正则表达式"],"title":"正则表达式入门","uri":"https://mayuanucas.github.io/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8/"},{"categories":["机器学习"],"content":"Apache Spark 是一个快速的，多用途的集群计算系统。它提供了 Java，Scala，Python 和 R 的高级 API，以及一个支持通用的执行图计算的优化过的引擎。它还支持一组丰富的高级工具，包括使用 SQL 处理结构化数据处理的 Spark SQL，用于机器学习的 MLlib，用于图计算的 GraphX，以及 Spark Streaming。 简介 Spark 于 2009 年诞生于加州大学伯克利分校 AMPLab，2013 年被捐赠给 Apache 软件基金会，2014 年 2 月成为 Apache 的顶级项目。相对于 MapReduce 的批处理计算，Spark 可以带来上百倍的性能提升，因此它成为继 MapReduce 之后，最为广泛使用的分布式计算框架。 特点 Apache Spark 具有以下特点： 使用先进的 DAG 调度程序，查询优化器和物理执行引擎，以实现性能上的保证； 多语言支持，目前支持的有 Java，Scala，Python 和 R； 提供高级 API，可以轻松地构建应用程序； 支持批处理，流处理和复杂的业务分析； 丰富的类库支持：包括 SQL，MLlib，GraphX 和 Spark Streaming 等库，并且可以将它们无缝地进行组合； 丰富的部署模式：支持本地模式和自带的集群模式，也支持在 Hadoop，Mesos，Kubernetes 上运行； 多数据源支持：支持访问 HDFS，Alluxio，Cassandra，HBase，Hive 以及数百个其他数据源中的数据。 集群架构 Term（术语） Meaning（含义） Application Spark 应用程序，由集群上的一个 Driver 节点和多个 Executor 节点组成。 Driver program 主运用程序，该进程运行应用的 main() 方法并且创建 SparkContext Cluster manager 集群资源管理器（例如，Standlone Manager，Mesos，YARN） Worker node 执行计算任务的工作节点 Executor 位于工作节点上的应用进程，负责执行计算任务并且将输出数据保存到内存或者磁盘中 Task 被发送到 Executor 中的工作单元 执行过程： 用户程序创建 SparkContext 后，它会连接到集群资源管理器，集群资源管理器会为用户程序分配计算资源，并启动 Executor； Dirver 将计算程序划分为不同的执行阶段和多个 Task，之后将 Task 发送给 Executor； Executor 负责执行 Task，并将执行状态汇报给 Driver，同时也会将当前节点资源的使用情况汇报给集群资源管理器。 核心组件 Spark 基于 Spark Core 扩展了四个核心组件，分别用于满足不同领域的计算需求。 ","date":"2020-03-09","objectID":"https://mayuanucas.github.io/spark/:0:0","tags":["Spark","scala"],"title":"Spark简明介绍","uri":"https://mayuanucas.github.io/spark/"},{"categories":["机器学习"],"content":"Spark SQL Spark SQL 主要用于结构化数据的处理。其具有以下特点： 能够将 SQL 查询与 Spark 程序无缝混合，允许您使用 SQL 或 DataFrame API 对结构化数据进行查询； 支持多种数据源，包括 Hive，Avro，Parquet，ORC，JSON 和 JDBC； 支持 HiveQL 语法以及用户自定义函数 (UDF)，允许你访问现有的 Hive 仓库； 支持标准的 JDBC 和 ODBC 连接； 支持优化器，列式存储和代码生成等特性，以提高查询效率。 ","date":"2020-03-09","objectID":"https://mayuanucas.github.io/spark/:1:0","tags":["Spark","scala"],"title":"Spark简明介绍","uri":"https://mayuanucas.github.io/spark/"},{"categories":["机器学习"],"content":"Spark Streaming Spark Streaming 主要用于快速构建可扩展，高吞吐量，高容错的流处理程序。支持从 HDFS，Flume，Kafka，Twitter 和 ZeroMQ 读取数据，并进行处理。 Spark Streaming 的本质是微批处理，它将数据流进行极小粒度的拆分，拆分为多个批处理，从而达到接近于流处理的效果。 ","date":"2020-03-09","objectID":"https://mayuanucas.github.io/spark/:2:0","tags":["Spark","scala"],"title":"Spark简明介绍","uri":"https://mayuanucas.github.io/spark/"},{"categories":["机器学习"],"content":"MLlib MLlib 是 Spark 的机器学习库。其设计目标是使得机器学习变得简单且可扩展。它提供了以下工具： 常见的机器学习算法：如分类，回归，聚类和协同过滤； 特征化：特征提取，转换，降维和选择； 管道：用于构建，评估和调整 ML 管道的工具； 持久性：保存和加载算法，模型，管道数据； 实用工具：线性代数，统计，数据处理等。 ","date":"2020-03-09","objectID":"https://mayuanucas.github.io/spark/:3:0","tags":["Spark","scala"],"title":"Spark简明介绍","uri":"https://mayuanucas.github.io/spark/"},{"categories":["机器学习"],"content":"Graphx GraphX 是 Spark 中用于图形计算和图形并行计算的新组件。在高层次上，GraphX 通过引入一个新的图形抽象来扩展 RDD(一种具有附加到每个顶点和边缘的属性的定向多重图形)。为了支持图计算，GraphX 提供了一组基本运算符（如：subgraph，joinVertices 和 aggregateMessages）以及优化后的 Pregel API。此外，GraphX 还包括越来越多的图形算法和构建器，以简化图形分析任务。 弹性式数据集RDDs RDD 全称为 Resilient Distributed Datasets，是 Spark 最基本的数据抽象，它是只读的、分区记录的集合，支持并行操作，可以由外部数据集或其他 RDD 转换而来，它具有以下特性： 一个 RDD 由一个或者多个分区（Partitions）组成。对于 RDD 来说，每个分区会被一个计算任务所处理，用户可以在创建 RDD 时指定其分区个数，如果没有指定，则默认采用程序所分配到的 CPU 的核心数； RDD 拥有一个用于计算分区的函数 compute； RDD 会保存彼此间的依赖关系，RDD 的每次转换都会生成一个新的依赖关系，这种 RDD 之间的依赖关系就像流水线一样。在部分分区数据丢失后，可以通过这种依赖关系重新计算丢失的分区数据，而不是对 RDD 的所有分区进行重新计算； Key-Value 型的 RDD 还拥有 Partitioner(分区器)，用于决定数据被存储在哪个分区中，目前 Spark 中支持 HashPartitioner(按照哈希分区) 和 RangeParationer(按照范围进行分区)； 一个优先位置列表 (可选)，用于存储每个分区的优先位置 (prefered location)。对于一个 HDFS 文件来说，这个列表保存的就是每个分区所在的块的位置，按照“移动数据不如移动计算“的理念，Spark 在进行任务调度的时候，会尽可能的将计算任务分配到其所要处理数据块的存储位置。 RDD[T] 抽象类的部分相关代码如下： // 由子类实现以计算给定分区 def compute(split: Partition, context: TaskContext): Iterator[T] // 获取所有分区 protected def getPartitions: Array[Partition] // 获取所有依赖关系 protected def getDependencies: Seq[Dependency[_]] = deps // 获取优先位置列表 protected def getPreferredLocations(split: Partition): Seq[String] = Nil // 分区器 由子类重写以指定它们的分区方式 @transient val partitioner: Option[Partitioner] = None Spark SQL ","date":"2020-03-09","objectID":"https://mayuanucas.github.io/spark/:4:0","tags":["Spark","scala"],"title":"Spark简明介绍","uri":"https://mayuanucas.github.io/spark/"},{"categories":["机器学习"],"content":"简介 Spark SQL 是 Spark 中的一个子模块，主要用于操作结构化数据。它具有以下特点： 能够将 SQL 查询与 Spark 程序无缝混合，允许您使用 SQL 或 DataFrame API 对结构化数据进行查询； 支持多种开发语言； 支持多达上百种的外部数据源，包括 Hive，Avro，Parquet，ORC，JSON 和 JDBC 等； 支持 HiveQL 语法以及 Hive SerDes 和 UDF，允许你访问现有的 Hive 仓库； 支持标准的 JDBC 和 ODBC 连接； 支持优化器，列式存储和代码生成等特性； 支持扩展并能保证容错。 ","date":"2020-03-09","objectID":"https://mayuanucas.github.io/spark/:5:0","tags":["Spark","scala"],"title":"Spark简明介绍","uri":"https://mayuanucas.github.io/spark/"},{"categories":["机器学习"],"content":"DataFrame \u0026 DataSet ","date":"2020-03-09","objectID":"https://mayuanucas.github.io/spark/:6:0","tags":["Spark","scala"],"title":"Spark简明介绍","uri":"https://mayuanucas.github.io/spark/"},{"categories":["机器学习"],"content":"DataFrame 为了支持结构化数据的处理，Spark SQL 提供了新的数据结构 DataFrame。DataFrame 是一个由具名列组成的数据集。它在概念上等同于关系数据库中的表或 R/Python 语言中的 dataframe。 由于 Spark SQL 支持多种语言的开发，所以每种语言都定义了 DataFrame 的抽象，主要如下： 语言 主要抽象 Scala Dataset[T] \u0026 DataFrame (Dataset[Row] 的别名) Java Dataset[T] Python DataFrame R DataFrame ","date":"2020-03-09","objectID":"https://mayuanucas.github.io/spark/:6:1","tags":["Spark","scala"],"title":"Spark简明介绍","uri":"https://mayuanucas.github.io/spark/"},{"categories":["机器学习"],"content":"DataFrame 对比 RDDs DataFrame 和 RDDs 最主要的区别在于一个面向的是结构化数据，一个面向的是非结构化数据，它们内部的数据结构如下： DataFrame 内部的有明确 Scheme 结构，即列名、列字段类型都是已知的，这带来的好处是可以减少数据读取以及更好地优化执行计划，从而保证查询效率。 DataFrame 和 RDDs 应该如何选择？ 如果你想使用函数式编程而不是 DataFrame API，则使用 RDDs； 如果你的数据是非结构化的 (比如流媒体或者字符流)，则使用 RDDs， 如果你的数据是结构化的 (如 RDBMS 中的数据) 或者半结构化的 (如日志)，出于性能上的考虑，应优先使用 DataFrame。 ","date":"2020-03-09","objectID":"https://mayuanucas.github.io/spark/:6:2","tags":["Spark","scala"],"title":"Spark简明介绍","uri":"https://mayuanucas.github.io/spark/"},{"categories":["机器学习"],"content":"DataSet Dataset 也是分布式的数据集合，在 Spark 1.6 版本被引入，它集成了 RDD 和 DataFrame 的优点，具备强类型的特点，同时支持 Lambda 函数，但只能在 Scala 和 Java 语言中使用。在 Spark 2.0 后，为了方便开发者，Spark 将 DataFrame 和 Dataset 的 API 融合到一起，提供了结构化的 API(Structured API)，即用户可以通过一套标准的 API 就能完成对两者的操作。 ","date":"2020-03-09","objectID":"https://mayuanucas.github.io/spark/:6:3","tags":["Spark","scala"],"title":"Spark简明介绍","uri":"https://mayuanucas.github.io/spark/"},{"categories":["机器学习"],"content":"静态类型与运行时类型安全 静态类型 (Static-typing) 与运行时类型安全 (runtime type-safety) 主要表现是：在实际使用中，如果你用的是 Spark SQL 的查询语句，则直到运行时你才会发现有语法错误，而如果你用的是 DataFrame 和 Dataset，则在编译时就可以发现错误 (这节省了开发时间和整体代价)。DataFrame 和 Dataset 主要区别在于： 在 DataFrame 中，当你调用了 API 之外的函数，编译器就会报错，但如果你使用了一个不存在的字段名字，编译器依然无法发现。而 Dataset 的 API 都是用 Lambda 函数和 JVM 类型对象表示的，所有不匹配的类型参数在编译时就会被发现。 以上这些最终都被解释成关于类型安全图谱，对应开发中的语法和分析错误。在图谱中，Dataset 最严格，但对于开发者来说效率最高。 ","date":"2020-03-09","objectID":"https://mayuanucas.github.io/spark/:6:4","tags":["Spark","scala"],"title":"Spark简明介绍","uri":"https://mayuanucas.github.io/spark/"},{"categories":["机器学习"],"content":"DataFrame \u0026 DataSet \u0026 RDDs 总结 这里对三者做一下简单的总结： RDDs 适合非结构化数据的处理，而 DataFrame \u0026 DataSet 更适合结构化数据和半结构化的处理； DataFrame \u0026 DataSet 可以通过统一的 Structured API 进行访问，而 RDDs 则更适合函数式编程的场景； 相比于 DataFrame 而言，DataSet 是强类型的 (Typed)，有着更为严格的静态类型检查； DataSets、DataFrames、SQL 的底层都依赖了 RDDs API，并对外提供结构化的访问接口。 ","date":"2020-03-09","objectID":"https://mayuanucas.github.io/spark/:6:5","tags":["Spark","scala"],"title":"Spark简明介绍","uri":"https://mayuanucas.github.io/spark/"},{"categories":["机器学习"],"content":"Spark SQL的运行原理 DataFrame、DataSet 和 Spark SQL 的实际执行流程都是相同的： 进行 DataFrame/Dataset/SQL 编程； 如果是有效的代码，即代码没有编译错误，Spark 会将其转换为一个逻辑计划； Spark 将此逻辑计划转换为物理计划，同时进行代码优化； Spark 然后在集群上执行这个物理计划 (基于 RDD 操作) 。 逻辑计划(Logical Plan) 执行的第一个阶段是将用户代码转换成一个逻辑计划。它首先将用户代码转换成 unresolved logical plan(未解决的逻辑计划)，之所以这个计划是未解决的，是因为尽管您的代码在语法上是正确的，但是它引用的表或列可能不存在。 Spark 使用 analyzer(分析器) 基于 catalog(存储的所有表和 DataFrames 的信息) 进行解析。解析失败则拒绝执行，解析成功则将结果传给 Catalyst 优化器 (Catalyst Optimizer)，优化器是一组规则的集合，用于优化逻辑计划，通过谓词下推等方式进行优化，最终输出优化后的逻辑执行计划。 物理计划(Physical Plan) 得到优化后的逻辑计划后，Spark 就开始了物理计划过程。 它通过生成不同的物理执行策略，并通过成本模型来比较它们，从而选择一个最优的物理计划在集群上面执行的。物理规划的输出结果是一系列的 RDDs 和转换关系 (transformations)。 执行 在选择一个物理计划后，Spark 运行其 RDDs 代码，并在运行时执行进一步的优化，生成本地 Java 字节码，最后将运行结果返回给用户。 ","date":"2020-03-09","objectID":"https://mayuanucas.github.io/spark/:6:6","tags":["Spark","scala"],"title":"Spark简明介绍","uri":"https://mayuanucas.github.io/spark/"},{"categories":["机器学习"],"content":"Structured API基本使用 创建DataFrame和Dataset 创建DataFrame Spark 中所有功能的入口点是 SparkSession，可以使用 SparkSession.builder() 创建。创建后应用程序就可以从现有 RDD，Hive 表或 Spark 数据源创建 DataFrame。示例如下： val spark = SparkSession.builder().appName(\"Spark-SQL\").master(\"local[2]\").getOrCreate() val df = spark.read.json(\"/usr/file/json/emp.json\") df.show() // 建议在进行 spark SQL 编程前导入下面的隐式转换，因为 DataFrames 和 dataSets 中很多操作都依赖了隐式转换 import spark.implicits._ 可以使用 spark-shell 进行测试，需要注意的是 spark-shell 启动后会自动创建一个名为 spark 的 SparkSession，在命令行中可以直接引用即可： 创建Dataset Spark 支持由内部数据集和外部数据集来创建 DataSet，其创建方式分别如下： 由外部数据集创建 // 1.需要导入隐式转换 import spark.implicits._ // 2.创建 case class,等价于 Java Bean case class Emp(ename: String, comm: Double, deptno: Long, empno: Long, hiredate: String, job: String, mgr: Long, sal: Double) // 3.由外部数据集创建 Datasets val ds = spark.read.json(\"/usr/file/emp.json\").as[Emp] ds.show() 由内部数据集创建 // 1.需要导入隐式转换 import spark.implicits._ // 2.创建 case class,等价于 Java Bean case class Emp(ename: String, comm: Double, deptno: Long, empno: Long, hiredate: String, job: String, mgr: Long, sal: Double) // 3.由内部数据集创建 Datasets val caseClassDS = Seq(Emp(\"ALLEN\", 300.0, 30, 7499, \"1981-02-20 00:00:00\", \"SALESMAN\", 7698, 1600.0), Emp(\"JONES\", 300.0, 30, 7499, \"1981-02-20 00:00:00\", \"SALESMAN\", 7698, 1600.0)) .toDS() caseClassDS.show() 由RDD创建DataFrame Spark 支持两种方式把 RDD 转换为 DataFrame，分别是使用反射推断和指定 Schema 转换： 使用反射推断 // 1.导入隐式转换 import spark.implicits._ // 2.创建部门类 case class Dept(deptno: Long, dname: String, loc: String) // 3.创建 RDD 并转换为 dataSet val rddToDS = spark.sparkContext .textFile(\"/usr/file/dept.txt\") .map(_.split(\"\\t\")) .map(line =\u003e Dept(line(0).trim.toLong, line(1), line(2))) .toDS() // 如果调用 toDF() 则转换为 dataFrame 以编程方式指定Schema import org.apache.spark.sql.Row import org.apache.spark.sql.types._ // 1.定义每个列的列类型 val fields = Array(StructField(\"deptno\", LongType, nullable = true), StructField(\"dname\", StringType, nullable = true), StructField(\"loc\", StringType, nullable = true)) // 2.创建 schema val schema = StructType(fields) // 3.创建 RDD val deptRDD = spark.sparkContext.textFile(\"/usr/file/dept.txt\") val rowRDD = deptRDD.map(_.split(\"\\t\")).map(line =\u003e Row(line(0).toLong, line(1), line(2))) // 4.将 RDD 转换为 dataFrame val deptDF = spark.createDataFrame(rowRDD, schema) deptDF.show() DataFrames与Datasets互相转换 Spark 提供了非常简单的转换方法用于 DataFrame 与 Dataset 间的互相转换，示例如下： # DataFrames转Datasets scala\u003e df.as[Emp] res1: org.apache.spark.sql.Dataset[Emp] = [COMM: double, DEPTNO: bigint ... 6 more fields] # Datasets转DataFrames scala\u003e ds.toDF() res2: org.apache.spark.sql.DataFrame = [COMM: double, DEPTNO: bigint ... 6 more fields] ","date":"2020-03-09","objectID":"https://mayuanucas.github.io/spark/:6:7","tags":["Spark","scala"],"title":"Spark简明介绍","uri":"https://mayuanucas.github.io/spark/"},{"categories":["机器学习"],"content":"Columns列操作 ","date":"2020-03-09","objectID":"https://mayuanucas.github.io/spark/:7:0","tags":["Spark","scala"],"title":"Spark简明介绍","uri":"https://mayuanucas.github.io/spark/"},{"categories":["机器学习"],"content":"引用列 Spark 支持多种方法来构造和引用列，最简单的是使用 col() 或 column() 函数。 col(\"colName\") column(\"colName\") // 对于 Scala 语言而言，还可以使用$\"myColumn\"和'myColumn 这两种语法糖进行引用。 df.select($\"ename\", $\"job\").show() df.select('ename, 'job).show() ","date":"2020-03-09","objectID":"https://mayuanucas.github.io/spark/:7:1","tags":["Spark","scala"],"title":"Spark简明介绍","uri":"https://mayuanucas.github.io/spark/"},{"categories":["机器学习"],"content":"新增列 // 基于已有列值新增列 df.withColumn(\"upSal\",$\"sal\"+1000) // 基于固定值新增列 df.withColumn(\"intCol\",lit(1000)) ","date":"2020-03-09","objectID":"https://mayuanucas.github.io/spark/:7:2","tags":["Spark","scala"],"title":"Spark简明介绍","uri":"https://mayuanucas.github.io/spark/"},{"categories":["机器学习"],"content":"删除列 // 支持删除多个列 df.drop(\"comm\",\"job\").show() ","date":"2020-03-09","objectID":"https://mayuanucas.github.io/spark/:7:3","tags":["Spark","scala"],"title":"Spark简明介绍","uri":"https://mayuanucas.github.io/spark/"},{"categories":["机器学习"],"content":"重命名列 df.withColumnRenamed(\"comm\", \"common\").show() 需要说明的是新增，删除，重命名列都会产生新的 DataFrame，原来的 DataFrame 不会被改变。 ","date":"2020-03-09","objectID":"https://mayuanucas.github.io/spark/:7:4","tags":["Spark","scala"],"title":"Spark简明介绍","uri":"https://mayuanucas.github.io/spark/"},{"categories":["机器学习"],"content":"使用Structured API进行基本查询 // 1.查询员工姓名及工作 df.select($\"ename\", $\"job\").show() // 2.filter 查询工资大于 2000 的员工信息 df.filter($\"sal\" \u003e 2000).show() // 3.orderBy 按照部门编号降序，工资升序进行查询 df.orderBy(desc(\"deptno\"), asc(\"sal\")).show() // 4.limit 查询工资最高的 3 名员工的信息 df.orderBy(desc(\"sal\")).limit(3).show() // 5.distinct 查询所有部门编号 df.select(\"deptno\").distinct().show() // 6.groupBy 分组统计部门人数 df.groupBy(\"deptno\").count().show() ","date":"2020-03-09","objectID":"https://mayuanucas.github.io/spark/:8:0","tags":["Spark","scala"],"title":"Spark简明介绍","uri":"https://mayuanucas.github.io/spark/"},{"categories":["机器学习"],"content":"使用Spark SQL进行基本查询 ","date":"2020-03-09","objectID":"https://mayuanucas.github.io/spark/:9:0","tags":["Spark","scala"],"title":"Spark简明介绍","uri":"https://mayuanucas.github.io/spark/"},{"categories":["机器学习"],"content":"Spark SQL基本使用 // 1.首先需要将 DataFrame 注册为临时视图 df.createOrReplaceTempView(\"emp\") // 2.查询员工姓名及工作 spark.sql(\"SELECT ename,job FROM emp\").show() // 3.查询工资大于 2000 的员工信息 spark.sql(\"SELECT * FROM emp where sal \u003e 2000\").show() // 4.orderBy 按照部门编号降序，工资升序进行查询 spark.sql(\"SELECT * FROM emp ORDER BY deptno DESC,sal ASC\").show() // 5.limit 查询工资最高的 3 名员工的信息 spark.sql(\"SELECT * FROM emp ORDER BY sal DESC LIMIT 3\").show() // 6.distinct 查询所有部门编号 spark.sql(\"SELECT DISTINCT(deptno) FROM emp\").show() // 7.分组统计部门人数 spark.sql(\"SELECT deptno,count(ename) FROM emp group by deptno\").show() ","date":"2020-03-09","objectID":"https://mayuanucas.github.io/spark/:9:1","tags":["Spark","scala"],"title":"Spark简明介绍","uri":"https://mayuanucas.github.io/spark/"},{"categories":["机器学习"],"content":"全局临时视图 上面使用 createOrReplaceTempView 创建的是会话临时视图，它的生命周期仅限于会话范围，会随会话的结束而结束。 你也可以使用 createGlobalTempView 创建全局临时视图，全局临时视图可以在所有会话之间共享，并直到整个 Spark 应用程序终止后才会消失。全局临时视图被定义在内置的 global_temp 数据库下，需要使用限定名称进行引用，如 SELECT * FROM global_temp.view1。 // 注册为全局临时视图 df.createGlobalTempView(\"gemp\") // 使用限定名称进行引用 spark.sql(\"SELECT ename,job FROM global_temp.gemp\").show() 参考文献 https://spark.apache.org/docs/latest/index.html https://spark.apache.org/docs/latest/sql-programming-guide.html ","date":"2020-03-09","objectID":"https://mayuanucas.github.io/spark/:9:2","tags":["Spark","scala"],"title":"Spark简明介绍","uri":"https://mayuanucas.github.io/spark/"},{"categories":["机器学习"],"content":"XGBoost是陈天奇于2014年提出的一套并行boost算法的工具库, LightGBM是微软推出的boosting框架, CatBoost是Yandex推出的Boost工具包. 本文将对这些算法进行介绍,并在数据集上对算法进行测试. XGBoost ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:0:0","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"简介 XGBoost的全称是eXtreme Gradient Boosting，既可以用于分类也可以用于回归问题中, 它是经过优化的分布式梯度提升库，旨在高效、灵活且可移植。XGBoost是大规模并行boosting tree的工具，它是目前最快最好的开源 boosting tree工具包，比常见的工具包快10倍以上。在数据科学方面，有大量的Kaggle选手选用XGBoost进行数据挖掘比赛，是各大数据科学比赛的必杀武器；在工业界大规模数据方面，XGBoost的分布式版本有广泛的可移植性，支持在Kubernetes、Hadoop、SGE、MPI、 Dask等各个分布式环境上运行，使得它可以很好地解决工业界大规模数据的问题。 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:1:0","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"什么是 Gradient Boosting Gradient boosting 是 boosting 的其中一种方法. 所谓 Boosting, 就是将弱分离器 $f_i(X)$ 组合起来形成强分类器 F(X) 的一种方法。 所以 Boosting 有三个要素: A loss function to be optimized: 例如分类问题中用 cross entropy，回归问题用 mean squared error A weak learner to make predictions: 例如决策树 An additive model: 将多个弱学习器累加起来组成强学习器，进而使目标损失函数达到极小 Gradient boosting 就是通过加入新的弱学习器，来努力纠正前面所有弱学习器的残差，最终这样多个学习器相加在一起用来进行最终预测，准确率就会比单独的一个要高。之所以称为 Gradient，是因为在添加新模型时使用了梯度下降算法来最小化的损失。 另一种 Gradient Boosting 的实现就是 AdaBoost（Adaptive Boosting）。 AdaBoost 就是将多个弱分类器，通过投票的手段来改变各个分类器的权值，使分错的分类器获得较大权值。同时在每一次循环中也改变样本的分布，这样被错误分类的样本也会受到更多的关注。 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:2:0","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"为什么使用 XGBoost XGBoost 是对 gradient boosting decision tree 的实现，但是一般来说，gradient boosting 的实现是比较慢的，因为每次都要先构造出一个树并添加到整个模型序列中。而 XGBoost 的特点就是计算速度快，模型表现好，这两点也正是项目的目标。 表现快是因为它具有这样的设计： Parallelization：训练时可以用所有的 CPU 内核来并行化建树。 Distributed Computing：用分布式计算来训练非常大的模型。 Out-of-Core Computing：对于非常大的数据集还可以进行 Out-of-Core Computing。 Cache Optimization of data structures and algorithms：更好地利用硬件资源。 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:3:0","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"应用 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:4:0","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"使用方式 使用xgboost库有两种方式： 第一种方式，直接使用xgboost库的建模流程 import xgboost as xgb # 第一步，读取数据 xgb.DMatrix() # 第二步，设置参数 param = {} # 第三步，训练模型 bst = xgb.train(param) # 第四步，预测结果 bst.predict() ​ 其中最核心的，是DMtarix这个读取数据的类，以及train()这个用于训练的类。与sklearn把所有的参数都写在类中的方式不同，xgboost库中必须先使用字典设定参数集，再使用train来将参数及输入，然后进行训练。会这样设计的原 因，是因为XGB所涉及到的参数实在太多，全部写在xgb.train()中太长也容易出错。 params可能的取值以及xgboost.train的参数如下： params {eta, gamma, max_depth, min_child_weight, max_delta_step, subsample, colsample_bytree, colsample_bylevel, colsample_bynode, lambda, alpha, tree_method string, sketch_eps, scale_pos_weight, updater, refresh_leaf, process_type, grow_policy, max_leaves, max_bin, predictor, num_parallel_tree} xgboost.train (params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None, maximize=False, early_stopping_rounds=None, evals_result=None, verbose_eval=True, xgb_model=None, callbacks=None, learning_rates=None) 第二种方式，使用xgboost库中的sklearn的API。 ​ 可以调用如下的类，并用 sklearn当中惯例的实例化，ﬁt和predict的流程来运行XGB，并且也可以调用属性比如coef_等等。 class xgboost.XGBRegressor (max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, objective='reg:linear', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, importance_type='gain', **kwargs) 调用xgboost.train和调用sklearnAPI中的类XGBRegressor，需要输入的参数是不同的，而且看起来相当的不同。但 其实，这些参数只是写法不同，功能是相同的。比如说，我们的params字典中的第一个参数eta，其实就是我们 XGBRegressor里面的参数learning_rate，他们的含义和实现的功能是一模一样的。只不过在sklearnAPI中，开发团 队友好地帮助我们将参数的名称调节成了与sklearn中其他的算法类更相似的样子。 ​ XGBoost本身的核心是基于梯度提升树实现的集成算法，整体来说可以有三个核心部分：集成算法本身，用于集成的弱评估器，以及应用中的其他过程。三个部分中，前两个部分包含了XGBoost的核心原理以及数学过程，最后的部分主要是在XGBoost应用中占有一席之地。 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:4:1","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"参数介绍 参数 集成算法 弱评估器 其他过程 n_estimators √ learning_rate √ silent √ subsample √ max_depth √ objective √ booster √ gamma √ min_child_weight √ max_delta_step √ colsample_bytree √ colsample_bylevel √ reg_alpha √ reg_lambda √ nthread √ n_jobs √ scale_pos_weight √ base_score √ seed √ andom_state √ missing √ importance_type √ 选择弱评估器：重要参数 booster 梯度提升算法中不只有梯度提升树，XGB作为梯度提升算法的进化，自然也不只有树模型一种弱评估器。在XGB中， 除了树模型，还可以选用线性模型，比如线性回归，来进行集成。虽然主流的XGB依然是树模型，但也可以 使用其他的模型。基于XGB的这种性质，有参数“booster\"来控制使用怎样的弱评估器。 xgb.train() \u0026 params xgb.XGBRegressor() xgb_model booster 使用哪种弱评估器。可以输入gbtree， gblinear或dart。输入的评估器不同，使用 的params参数也不同，每种评估器都有自 己的params列表。评估器必须于param参 数相匹配，否则报错。 使用哪种弱评估器。可以输入gbtree，gblinear或dart。 gbtree代表梯度提升树，dart是Dropouts meet Multiple Additive Regression Trees，可译为抛弃提升树，在建树的过程中会抛弃一部分树，比梯度提升树有更好的防过拟合功能，输入gblinear使用线性模型。 两个参数都默认为\"gbtree\"，如果不想使用树模型，则可以自行调整。当XGB使用线性模型的时候，它的许多数学过程就与使用普通的Boosting集成非常相似。 XGB的目标函数：重要参数objective 梯度提升算法中都存在着损失函数。不同于逻辑回归和SVM等算法中固定的损失函数写法，集成算法中的损失函数是可选的，要选用什么损失函数取决于我们希望解决什么问题，以及希望使用怎样的模型。比如说，如果我们的目标是进行回归预测，那我们可以选择调节后的均方误差RMSE作为我们的损失函数。如果我们是进行分类预测，那我们可以选择错误率error或者对数损失log_loss。只要我们选出的函数是一个可微的，能够代表某种损失的函数，它就可以是我们XGB中的损失函数。 在众多机器学习算法中，损失函数的核心是衡量模型的泛化能力，即模型在未知数据上的预测的准确与否，我们训练模型的核心目标也是希望模型能够预测准确。在XGB中，准确预测自然是非常重要的因素，但需注意，XGB 是实现了模型表现和运算速度的平衡的算法。普通的损失函数，比如错误率，均方误差等，都只能够衡量模型的表现，无法衡量模型的运算速度。许多模型中使用空间复杂度和时间复杂度来衡量模型的运算效 率。XGB因此引入了模型复杂度来衡量算法的运算效率。因此XGB的目标函数被写作：传统损失函数 + 模型复杂度。 $$ Obj = \\sum_{i=1}^ml(y_i,\\hat{y_i}) + \\sum_{k=1}^K\\Omega(f_k) $$ 其中$i$代表数据集中的第 $i$ 个样本，$m$ 表示导入第 $k$ 棵树的数据总量，$K$ 代表建立的所有树(n_estimators)，当只建立了 $t$ 棵树的时候，式子应当为 $\\sum_{k=1}^t\\Omega(f_k)$。第一项代表传统的损失函数，衡量真实标签 $y_i$ 与预测值 $\\hat{y_i}$ 之间的差异，通常是RMSE调节后的均方误差。第二项代表模型的复杂度，使用树模型的某种变换 $\\Omega$ 表示，这个变化代表了一个从树的结构来衡量树模型的复杂度的式子，可以有多种定义。注意，我们的第二项中没有特征矩阵 $x_i$ 的介入。我们在迭代 每一棵树的过程中，都最小化 Obj 来力求获取最优的 $\\hat{y}$ ，因此我们同时最小化了模型的错误率和模型的复杂度，这种 设计目标函数的方法不得不说实在是非常巧妙和聪明。 还可以从另一个角度去理解目标函数,即方差-偏差困境。在机器学习中，用来衡量模型在未知数据上的准确率的指标，叫做泛化误差（Genelization error）。一个集成模型(f) 在未知数据集(D)上的泛化误差 $E(f;D)$ ，由方差(var)，偏差(bais)和噪声 $\\varepsilon$共同决定，泛化误差越小，模型就越理想。从下面的图可以看出来，方差和偏差是此消彼长的，并且模型的复杂度越高，方差越大，偏差越小。 方差可以被简单地解释为模型在不同数据集上表现出来地稳定性，而偏差是模型预测的准确度。那方差-偏差困境就可以对应到 Obj 中了: $$ Obj = \\sum_{i=1}^ml(y_i,\\hat{y_i}) + \\sum_{k=1}^K\\Omega(f_k) $$ 第一项是衡量我们模型的偏差，模型越不准确，第一项就会越大。第二项是衡量我们的方差，模型越复杂，模型的学习就会越具体，到不同数据集上的表现就会差异巨大，方差就会越大。所以我们求解的最小值，其实是在求解方差与偏差的平衡点，以求模型的泛化误差最小，运行速度最快。我们知道树模型和树的集成模型都是学习天才，是天生过拟合的模型，因此大多数树模型最初都会出现在图像的右上方，我们必须通过剪枝来控制模型不要过拟合。现在 XGBoost的损失函数中自带限制方差变大的部分，也就是说XGBoost会比其他的树模型更加聪明，不会轻易落到图像的右上方。可见，这个模型在设计的时候的确是考虑了方方面面，难怪XGBoost会如此强大了。 在应用中，我们使用参数“objective\"来确定我们目标函数的第一部分中的 $l(y_i, \\hat{y_i})$ ，也就是衡量损失的部分。 xgb.train() xgb.XGBRegressor() Xgb.XGBClassifier() obj: 默认binary:logistic objective: 默认reg:linear objective: 默认binary:logistic 常用的选择有： 输入 选用的损失函数 reg:linear 使用线性回归的损失函数，均方误差，回归时使用 binary:logistic 使用逻辑回归的损失函数，对数损失log_loss，二分类时使用 binary:logistic 使用支持向量机的损失函数，Hinge Loss，二分类时使用 multi:softmax 使用softmax损失函数，多分类时使用 还可以选择自定义损失函数。比如说，我们可以选择输入平方损失 $l(y_i, \\hat{y_i} = (y_i - \\hat{y_i})^2)$，此时XGBoost 其实就是算法梯度提升机器（gradient boosted machine）。在xgboost中，我们被允许自定义损失函数，但通常我们还是使用类已经为我们设置好的损失函数。回归类中本来使用的就是reg:linear，因此在这里无需做任何调整。注意：分类型的目标函数导入回归类中会直接报错。现在来试试看xgb自身的调用方式。 由于xgb中所有的参数都需要自己的输入，并且objective参数的默认值是二分类，因此我们必须手动调节。 正则化参数 对每一棵树，它都有自己独特的结构，这个结构即是指叶子节点的数量，树的深度，叶子的位置等等所形成的一个可以定义唯一模型的树结构。在这个结构中，我们使用 $q(x_i)$表示样本$x_i$ 所在的叶子节点，并且使用$w_{q(x_i)}$ 来表示这个样本落到第 t棵树上的第$q(x_i)$ 个叶子节点中所获得的分数，于是有： $$ f_t(x_i) = w_{q(x_i)} $$ 这是对于每一个样本而言的叶子权重，然而在一个叶子节点上的所有样本所对应的叶子权重是相同的。设一棵树上总共包含了T个叶子节点，其中每个叶子节点的索引为j ，则这个叶子节点上的样本权重是$w_j$ 。依据这个，我们定义模型的复杂度$\\Omega(f)$为（注意这不是唯一可能的定义，我们当然还可以使用其他的定义，只要满足叶子越多/深度越大， 复杂度越大的理论，可以自己决定要是一个怎样的式子）： $$ \\Omega(f) = \\gamma T + 正则项(Regularization) $$ 如果使用$L2$正则项： $$ = \\gamma T + \\frac 1{2} \\lambda|w|^2 $$ $$ =\\gamma T + \\frac 1{2} \\lambda \\sum_{j=1}^T w_j^2 $$ 如果使用$L1$正则项： $$ = \\gamma T + \\frac 1{2} \\alpha|w| $$ $$ =\\gamma T + \\frac 1{2} \\alpha \\sum_{j=1}^T |w_j| $$ 还可以两个一起使用： $$ =\\gamma T + \\frac 1{2} \\alpha \\sum_{j=1}^T |w_j| + \\frac 1{2} \\lambda \\sum_{j=1}^T w_j^2 $$ 这个结构中有两部分内容，一部分是控制树结构的$\\gamma T$ ，另一部分则是我们的正则项。叶子数量 可以代表整个树结构，这是因为在X","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:4:2","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"简介 GBDT (Gradient Boosting Decision Tree) 是机器学习中一个长盛不衰的模型，其主要思想是利用弱分类器（决策树）迭代训练以得到最优模型，该模型具有训练效果好、不易过拟合等优点。GBDT 不仅在工业界应用广泛，通常被用于多分类、点击率预测、搜索排序等任务；在各种数据挖掘竞赛中也是致命武器，据统计Kaggle上的比赛有一半以上的冠军方案都是基于 GBDT。而 LightGBM（Light Gradient Boosting Machine）是一个实现 GBDT 算法的框架，支持高效率的并行训练，并且具有更快的训练速度、更低的内存消耗、更好的准确率、支持分布式可以快速处理海量数据等优点。 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:5:0","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"为什么使用LightGBM 常用的机器学习算法，例如神经网络等算法，都可以以mini-batch的方式训练，训练数据的大小不会受到内存限制。而GBDT在每一次迭代的时候，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。尤其面对工业级海量的数据，普通的GBDT算法是不能满足其需求的。 LightGBM提出的主要原因就是为了解决GBDT在海量数据遇到的问题，让GBDT可以更好更快地用于工业实践。 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:6:0","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"XGBoost的不足及LightGBM的优化 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:7:0","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"XGBoost的不足 在LightGBM提出之前，最有名的GBDT工具就是XGBoost了，它是基于预排序方法的决策树算法。这种构建决策树的算法基本思想是：首先，对所有特征都按照特征的数值进行预排序。其次，在遍历分割点的时候用 O(data) 的代价找到一个特征上的最好分割点。最后，在找到一个特征的最好分割点的条件下，将数据分裂成左右子节点。 这样的预排序算法的优点是能精确地找到分割点。但是缺点也很明显：首先，空间消耗大。这样的算法需要保存数据的特征值，还保存了特征排序的结果（例如，为了后续快速的计算分割点，保存了排序后的索引），这就需要消耗训练数据两倍的内存。其次，时间上也有较大的开销，在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。最后，对cache优化不友好。在预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对cache进行优化。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的cache miss。 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:7:1","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"LightGBM的优化 为了避免上述XGBoost的缺陷，并且能够在不损害准确率的条件下加快GBDT模型的训练速度，lightGBM在传统的GBDT算法上进行了如下优化： 基于Histogram的决策树算法 单边梯度采样 Gradient-based One-Side Sampling(GOSS)：使用GOSS可以减少大量只具有小梯度的数据实例，这样在计算信息增益的时候只利用剩下的具有高梯度的数据就可以了，相比XGBoost遍历所有特征值节省了不少时间和空间上的开销 互斥特征捆绑 Exclusive Feature Bundling(EFB)：使用EFB可以将许多互斥的特征绑定为一个特征，这样达到了降维的目的 带深度限制的Leaf-wise的叶子生长策略：大多数GBDT工具使用低效的按层生长 (level-wise) 的决策树生长策略，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销。实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。LightGBM使用了带有深度限制的按叶子生长 (leaf-wise) 算法 直接支持类别特征(Categorical Feature) 支持高效并行 Cache命中率优化 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:7:2","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"LightGBM的优缺点 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:8:0","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"优点 速度更快 LightGBM 采用了直方图算法将遍历样本转变为遍历直方图，极大的降低了时间复杂度 LightGBM 在训练过程中采用单边梯度算法过滤掉梯度小的样本，减少了大量的计算 LightGBM 采用了基于 Leaf-wise 算法的增长策略构建树，减少了很多不必要的计算量 LightGBM 采用优化后的特征并行、数据并行方法加速计算，当数据量非常大的时候还可以采用投票并行的策略 LightGBM 对缓存也进行了优化，增加了缓存命中率 内存更小 XGBoost使用预排序后需要记录特征值及其对应样本的统计值的索引，而 LightGBM 使用了直方图算法将特征值转变为 bin 值，且不需要记录特征到样本的索引，将空间复杂度从 O(2*data) 降低为 O(bin) ，极大的减少了内存消耗 LightGBM 采用了直方图算法将存储特征值转变为存储 bin 值，降低了内存消耗 LightGBM 在训练过程中采用互斥特征捆绑算法减少了特征数量，降低了内存消耗 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:8:1","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"缺点 可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度限制，在保证高效率的同时防止过拟合 Boosting族是迭代算法，每一次迭代都根据上一次迭代的预测结果对样本进行权重调整，所以随着迭代不断进行，误差会越来越小，模型的偏差（bias）会不断降低。由于LightGBM是基于偏差的算法，所以会对噪点较为敏感 在寻找最优解时，依据的是最优切分变量，没有将最优解是全部特征的综合这一理念考虑进去 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:8:2","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"应用 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:9:0","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"安装LightGBM依赖包 pip install lightgbm ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:9:1","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"LightGBM分类和回归 LightGBM有两大类接口：LightGBM原生接口 和 scikit-learn接口 ，并且LightGBM能够实现分类和回归两种任务。 基于LightGBM原生接口的分类 import numpy as np from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.metrics import roc_auc_score, accuracy_score import lightgbm as lgb # 加载数据 iris = datasets.load_iris() # 划分训练集和测试集 X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3) # 转换为Dataset数据格式 train_data = lgb.Dataset(X_train, label=y_train) validation_data = lgb.Dataset(X_test, label=y_test) # 参数 params = { 'learning_rate': 0.1, 'lambda_l1': 0.1, 'lambda_l2': 0.2, 'max_depth': 4, 'objective': 'multiclass', # 目标函数 'num_class': 3, } # 模型训练 gbm = lgb.train(params, train_data, valid_sets=[validation_data]) # 模型预测 y_pred = gbm.predict(X_test) y_pred = [list(x).index(max(x)) for x in y_pred] print(y_pred) # 模型评估 print(accuracy_score(y_test, y_pred)) 基于Scikit-learn接口的分类 from lightgbm import LGBMClassifier from sklearn.metrics import accuracy_score from sklearn.model_selection import GridSearchCV from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.externals import joblib # 加载数据 iris = load_iris() data = iris.data target = iris.target # 划分训练数据和测试数据 X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2) # 模型训练 gbm = LGBMClassifier(num_leaves=31, learning_rate=0.05, n_estimators=20) gbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=5) # 模型存储 joblib.dump(gbm, 'loan_model.pkl') # 模型加载 gbm = joblib.load('loan_model.pkl') # 模型预测 y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration_) # 模型评估 print('The accuracy of prediction is:', accuracy_score(y_test, y_pred)) # 特征重要度 print('Feature importances:', list(gbm.feature_importances_)) # Feature importances: [28, 6, 97, 61] # 网格搜索，参数优化 estimator = LGBMClassifier(num_leaves=31) param_grid = { 'learning_rate': [0.01, 0.1, 1], 'n_estimators': [20, 40] } gbm = GridSearchCV(estimator, param_grid) gbm.fit(X_train, y_train) print('Best parameters found by grid search are:', gbm.best_params_) # Best parameters found by grid search are: {'learning_rate': 0.1, 'n_estimators': 20} 基于LightGBM原生接口的回归 对于LightGBM解决回归问题，我们用Kaggle比赛中回归问题：House Prices: Advanced Regression Techniques，地址：https://www.kaggle.com/c/house-prices-advanced-regression-techniques 来进行实例讲解。 该房价预测的训练数据集中一共有列，第一列是Id，最后一列是label，中间列是特征。这列特征中，有列是分类型变量，列是整数变量，列是浮点型变量。训练数据集中存在缺失值。 import pandas as pd from sklearn.model_selection import train_test_split import lightgbm as lgb from sklearn.metrics import mean_absolute_error from sklearn.preprocessing import Imputer # 1.读文件 data = pd.read_csv('./dataset/train.csv') # 2.切分数据输入：特征 输出：预测目标变量 y = data.SalePrice X = data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object']) # 3.切分训练集、测试集,切分比例7.5 : 2.5 train_X, test_X, train_y, test_y = train_test_split(X.values, y.values, test_size=0.25) # 4.空值处理，默认方法：使用特征列的平均值进行填充 my_imputer = Imputer() train_X = my_imputer.fit_transform(train_X) test_X = my_imputer.transform(test_X) # 5.转换为Dataset数据格式 lgb_train = lgb.Dataset(train_X, train_y) lgb_eval = lgb.Dataset(test_X, test_y, reference=lgb_train) # 6.参数 params = { 'task': 'train', 'boosting_type': 'gbdt', # 设置提升类型 'objective': 'regression', # 目标函数 'metric': {'l2', 'auc'}, # 评估函数 'num_leaves': 31, # 叶子节点数 'learning_rate': 0.05, # 学习速率 'feature_fraction': 0.9, # 建树的特征选择比例 'bagging_fraction': 0.8, # 建树的样本采样比例 'bagging_freq': 5, # k 意味着每 k 次迭代执行bagging 'verbose': 1 # \u003c0 显示致命的, =0 显示错误 (警告), \u003e0 显示信息 } # 7.调用LightGBM模型，使用训练集数据进行训练（拟合） # Add verbosity=2 to print messages while running boosting my_model = lgb.train(params, lgb_train, num_boost_round=20, valid_sets=lgb_eval, early_stopping_rounds=5) # 8.使用模型对测试集数据进行预测 predictions = my_model.predict(test_X, num_iteration=my_model.best_iteration) # 9.对模型的预测结果进行评判（平均绝对误差） print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y))) # Mean Absolute Error : 55355.984107934746 基于Scikit-learn接口的回归 impo","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:9:2","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"LightGBM调参 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:10:0","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"控制参数 Control Parameters 含义 用法 max_depth 树的最大深度 当模型过拟合时,可以考虑首先降低 max_depth min_data_in_leaf 叶子可能具有的最小记录数 默认20，过拟合时用 feature_fraction 例如 为0.8时，意味着在每次迭代中随机选择80％的参数来建树 boosting 为 random forest 时用 bagging_fraction 每次迭代时用的数据比例 用于加快训练速度和减小过拟合 early_stopping_round 如果一次验证数据的一个度量在最近的early_stopping_round 回合中没有提高，模型将停止训练 加速分析，减少过多迭代 lambda 指定正则化 0～1 min_gain_to_split 描述分裂的最小 gain 控制树的有用的分裂 max_cat_group 在 group 边界上找到分割点 当类别数量很多时，找分割点很 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:10:1","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"核心参数 CoreParameters 含义 用法 Task 数据的用途 选择 train 或者 predict application 模型的用途 选择 regression: 回归时，binary: 二分类时，multiclass: 多分类时 boosting 要用的算法 gbdt， rf: random forest， dart: Dropouts meet Multiple Additive Regression Trees， goss: Gradient-based One-Side Sampling num_boost_round 迭代次数 通常 100+ learning_rate 如果一次验证数据的一个度量在最近的 early_stopping_round 回合中没有提高，模型将停止训练 常用 0.1, 0.001, 0.003… num_leaves 默认 31 device cpu 或者 gpu metric mae: mean absolute error ， mse: mean squared error ， binary_logloss: loss for binary classification ， multi_logloss: loss for multi classification ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:10:2","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"IO参数 IO parameter 含义 max_bin 表示 feature 将存入的 bin 的最大数量 categorical_feature 如果 categorical_features = 0,1,2， 则列 0，1，2是 categorical 变量 ignore_column 与 categorical_features 类似，只不过不是将特定的列视为categorical，而是完全忽略 save_binary 这个参数为 true 时，则数据集被保存为二进制文件，下次读数据时速度会变快 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:10:3","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"调参 IO parameter 含义 num_leaves 取值应 \u003c= 2 ^（max_depth）， 超过此值会导致过拟合 min_data_in_leaf 将它设置为较大的值可以避免生长太深的树，但可能会导致 underfitting，在大型数据集时就设置为数百或数千 max_depth 这个也是可以限制树的深度 下表对应了 Faster Speed ，better accuracy ，over-fitting 三种目的时，可以调的参数： Faster Speed better accuracy over-fitting 将 max_bin 设置小一些 用较大的 max_bin max_bin 小一些 num_leaves 大一些 num_leaves 小一些 用 feature_fraction 来做 sub-sampling 用 feature_fraction 用 bagging_fraction 和 bagging_freq 设定 bagging_fraction 和 bagging_freq training data 多一些 training data 多一些 用 save_binary 来加速数据加载 直接用 categorical feature 用 gmin_data_in_leaf 和 min_sum_hessian_in_leaf 用 parallel learning 用 dart 用 lambda_l1, lambda_l2 ，min_gain_to_split 做正则化 num_iterations 大一些，learning_rate 小一些 用 max_depth 控制树的深度 CatBoost ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:10:4","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"简介 CatBoost是俄罗斯的搜索巨头 Yandex 在2017年开源的机器学习库，也是Boosting族算法的一种，同前面介绍过的XGBoost和LightGBM类似，依然是在GBDT算法框架下的一种改进实现，是一种基于对称决策树（oblivious trees）算法的参数少、支持类别型变量和高准确性的GBDT框架，主要说解决的痛点是高效合理地处理类别型特征，这个从它的名字就可以看得出来，CatBoost是由catgorical和boost组成，另外是处理梯度偏差（Gradient bias）以及预测偏移（Prediction shift）问题，提高算法的准确性和泛化能力。 与XGBoost、LightGBM相比，CatBoost的创新点有： 嵌入了自动将类别型特征处理为数值型特征的创新算法。首先对categorical features做一些统计，计算某个类别特征（category）出现的频率，之后加上超参数，生成新的数值型特征（numerical features）。 Catboost还使用了组合类别特征，可以利用到特征之间的联系，这极大的丰富了特征维度。 采用排序提升的方法对抗训练集中的噪声点，从而避免梯度估计的偏差，进而解决预测偏移的问题。 采用了完全对称树作为基模型。 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:11:0","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"类别型特征 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:12:0","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"类别型特征的相关工作 所谓类别型特征，即这类特征不是数值型特征，而是离散的集合，比如省份名（山东、山西、河北等），城市名（北京、上海、深圳等），学历（本科、硕士、博士等）。在梯度提升算法中，最常用的是将这些类别型特征转为数值型来处理，一般类别型特征会转化为一个或多个数值型特征。 如果某个类别型特征基数比较低（low-cardinality features），即该特征的所有值去重后构成的集合元素个数比较少，一般利用One-hot编码方法将特征转为数值型。One-hot编码可以在数据预处理时完成，也可以在模型训练的时候完成，从训练时间的角度，后一种方法的实现更为高效，CatBoost对于基数较低的类别型特征也是采用后一种实现。 显然，在高基数类别型特征（high cardinality features） 当中，比如 user ID，这种编码方式会产生大量新的特征，造成维度灾难。一种折中的办法是可以将类别分组成有限个的群体再进行One-hot编码。一种常被使用的方法是根据目标变量统计（Target Statistics，以下简称TS）进行分组，目标变量统计用于估算每个类别的目标变量期望值。甚至有人直接用TS作为一个新的数值型变量来代替原来的类别型变量。重要的是，可以通过对TS数值型特征的阈值设置，基于对数损失、基尼系数或者均方差，得到一个对于训练集而言将类别一分为二的所有可能划分当中最优的那个。在LightGBM当中，类别型特征用每一步梯度提升时的梯度统计（Gradient Statistics，以下简称GS）来表示。虽然为建树提供了重要的信息，但是这种方法有以下两个缺点： 增加计算时间，因为需要对每一个类别型特征，在迭代的每一步，都需要对GS进行计算； 增加存储需求，对于一个类别型变量，需要存储每一次分离每个节点的类别； 为了克服这些缺点，LightGBM以损失部分信息为代价将所有的长尾类别归为一类，作者声称这样处理高基数类别型特征时比One-hot编码还是好不少。不过如果采用TS特征，那么对于每个类别只需要计算和存储一个数字。 因此，采用TS作为一个新的数值型特征是最有效、信息损失最小的处理类别型特征的方法。TS也被广泛应用在点击预测任务当中，这个场景当中的类别型特征有用户、地区、广告、广告发布者等。接下来着重讨论TS，暂时将One-hot编码和GS放一边。 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:12:1","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"目标变量统计（Target Statistics） CatBoost算法的设计初衷是为了更好的处理GBDT特征中的categorical features。在处理 GBDT特征中的categorical features的时候，最简单的方法是用 categorical feature 对应的标签的平均值来替换。 在决策树中，标签平均值将作为节点分裂的标准。这种方法被称为 Greedy Target-based Statistics , 简称 Greedy TS，用公式来表达就是： $$ \\hat x_k^{i} = \\frac {\\sum_{j=1}^n[x_{j,k} = x_{i,k}] * Y_i}{\\sum_{j=1}^n[x_{j,k} = x_{i,k}]} $$ 这种方法有一个显而易见的缺陷，就是通常特征比标签包含更多的信息，如果强行用标签的平均值来表示特征的话，当训练数据集和测试数据集数据结构和分布不一样的时候会出条件偏移问题。 一个标准的改进 Greedy TS的方式是添加先验分布项，这样可以减少噪声和低频率类别型数据对于数据分布的影响： $$ \\hat x_k^i = \\frac {\\sum_{j=1}^{p-1}[x_{\\sigma_{j,k}} = x_{\\sigma_{p,k}}]Y_{\\sigma_j} + a * p}{\\sum_{j=1}^{p-1}[x_{\\sigma_{j,k}} = x_{\\sigma_{p,k}}] + a} $$ 其中 p 是添加的先验项， a 通常是大于 0 的权重系数。添加先验项是一个普遍做法，针对类别数较少的特征，它可以减少噪声数据。对于回归问题，一般情况下，先验项可取数据集label的均值。对于二分类，先验项是正例的先验概率。利用多个数据集排列也是有效的，但是，如果直接计算可能导致过拟合。CatBoost利用了一个比较新颖的计算叶子节点值的方法，这种方式（oblivious trees，对称树）可以避免多个数据集排列中直接计算会出现过拟合的问题。 当然，在论文《CatBoost: unbiased boosting with categorical features》中，还提到了其它几种改进Greedy TS的方法，分别有：Holdout TS、Leave-one-out TS、Ordered TS。 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:12:2","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"特征组合 值得注意的是几个类别型特征的任意组合都可视为新的特征。例如，在音乐推荐应用中，我们有两个类别型特征：用户ID和音乐流派。如果有些用户更喜欢摇滚乐，将用户ID和音乐流派转换为数字特征时，根据上述这些信息就会丢失。结合这两个特征就可以解决这个问题，并且可以得到一个新的强大的特征。然而，组合的数量会随着数据集中类别型特征的数量成指数增长，因此不可能在算法中考虑所有组合。为当前树构造新的分割点时，CatBoost会采用贪婪的策略考虑组合。对于树的第一次分割，不考虑任何组合。对于下一个分割，CatBoost将当前树的所有组合、类别型特征与数据集中的所有类别型特征相结合，并将新的组合类别型特征动态地转换为数值型特征。CatBoost还通过以下方式生成数值型特征和类别型特征的组合：树中选定的所有分割点都被视为具有两个值的类别型特征，并像类别型特征一样被进行组合考虑。 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:12:3","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"CatBoost处理Categorical features总结 首先会计算一些数据的statistics。计算某个category出现的频率，加上超参数，生成新的numerical features。这一策略要求同一标签数据不能排列在一起（即先全是0之后全是1这种方式），训练之前需要打乱数据集。 第二，使用数据的不同排列（实际上是4个）。在每一轮建立树之前，先扔一轮骰子，决定使用哪个排列来生成树。 第三，考虑使用categorical features的不同组合。例如颜色和种类组合起来，可以构成类似于blue dog这样的特征。当需要组合的categorical features变多时，CatBoost只考虑一部分combinations。在选择第一个节点时，只考虑选择一个特征，例如A。在生成第二个节点时，考虑A和任意一个categorical feature的组合，选择其中最好的。就这样使用贪心算法生成combinations。 第四，除非向gender这种维数很小的情况，不建议自己生成One-hot编码向量，最好交给算法来处理。 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:12:4","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"克服梯度偏差 对于学习CatBoost克服梯度偏差的内容，提出了三个问题： 为什么会有梯度偏差？ 梯度偏差造成了什么问题？ 如何解决梯度偏差？ CatBoost和所有标准梯度提升算法一样，都是通过构建新树来拟合当前模型的梯度。然而，所有经典的提升算法都存在由有偏的点态梯度估计引起的过拟合问题。在每个步骤中使用的梯度都使用当前模型中的相同的数据点来估计，这导致估计梯度在特征空间的任何域中的分布与该域中梯度的真实分布相比发生了偏移，从而导致过拟合。为了解决这个问题，CatBoost对经典的梯度提升算法进行了一些改进，简要介绍如下。 许多利用GBDT技术的算法（例如，XGBoost、LightGBM），构建下一棵树分为两个阶段：选择树结构和在树结构固定后计算叶子节点的值。为了选择最佳的树结构，算法通过枚举不同的分割，用这些分割构建树，对得到的叶子节点计算值，然后对得到的树计算评分，最后选择最佳的分割。两个阶段叶子节点的值都是被当做梯度或牛顿步长的近似值来计算。在CatBoost中，第一阶段采用梯度步长的无偏估计，第二阶段使用传统的GBDT方案执行。既然原来的梯度估计是有偏的，那么怎么能改成无偏估计呢？ 设 $F_i$ 为构建i 棵树后的模型，$g^i(X_k,Y_k)$ 为构建 i棵树后第k 个训练样本上面的梯度值。为了使得$g^i(X_k,Y_k)$ 无偏于模型 $F_i$ ，我们需要在没有$X_k$ 参与的情况下对模型 进行训练。由于我们需要对所有训练样本计算无偏的梯度估计，乍看起来对于$F_i$ 的训练不能使用任何样本，貌似无法实现的样子。我们运用下面这个技巧来处理这个问题：对于每一个样本 $X_k$，我们训练一个单独的模型 $M_k$，且该模型从不使用基于该样本的梯度估计进行更新。我们使用$M_k$ 来估计 $X_k$ 上的梯度，并使用这个估计对结果树进行评分。用伪码描述如下，其中 $Loss(y_i, a)$ 是需要优化的损失函数， y 是标签值， a 是公式计算值。 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:13:0","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"预测偏移和排序提升 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:14:0","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"预测偏移 对于学习预测偏移的内容，提出了两个问题： 什么是预测偏移？ 用什么办法解决预测偏移问题？ 预测偏移（Prediction shift）是由梯度偏差造成的。在GDBT的每一步迭代中, 损失函数使用相同的数据集求得当前模型的梯度, 然后训练得到基学习器, 但这会导致梯度估计偏差, 进而导致模型产生过拟合的问题。CatBoost通过采用排序提升 （Ordered boosting） 的方式替换传统算法中梯度估计方法，进而减轻梯度估计的偏差，提高模型的泛化能力。下面我们对预测偏移进行详细的描述和分析。 首先来看下GBDT的整体迭代过程： GBDT算法是通过一组分类器的串行迭代，最终得到一个强学习器，以此来进行更高精度的分类。它使用了前向分布算法，弱学习器使用分类回归树（CART）。 假设前一轮迭代得到的强学习器是 $F^{t-1}(x)$ , 损失函数是$L(y, F^{t-1}(x))$ ，则本轮迭代的目的是找到一个CART回归树模型的弱学习器 $h^t$，让本轮的损失函数最小。下面的式子表示的是本轮迭代的目标函数 $h^t$ 。 $$ h^t = \\arg \\min_{h\\in H}EL((y,F^{t-1}(x) + h(x))) $$ GBDT使用损失函数的负梯度来拟合每一轮的损失的近似值，下面式子中$g^t(x,y)$ 表示的是上述梯度。 $$ g^t(x,y) = \\frac {\\partial L(y,s)}{\\partial s}|_{s=F^{t-1}(x)} $$ 通常用下式近似拟合$h^t$. $$ h^t = \\arg \\min_{h\\in H}E(-g^t(x,y) - h(x))^2 $$ 最终得到本轮的强学习器，如式（4）所示： $$ F^t(x) = F^{t-1}(x) + h^t $$ **在这个过程当中，偏移是这样发生的：** 根据 D\\${X_k}$ 进行随机计算的条件分布$g^t(X_k, y_k)|X_k$ 与测试集的分布$g^t(X,y)|X$ 发生偏移，这样由公式（3）定义的基学习器 与公式（1）定义的产生偏差，最后影响模型 $F^t$ 的泛化能力。 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:14:1","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"排序提升 为了克服预测偏移问题，CatBoost提出了一种新的叫做Ordered boosting的算法。 由上图的Ordered boosting算法可知，为了得到无偏梯度估计, CatBoost对每一个样本 都会训练一个单独的模型 ，模型 由使用不包含样本的训练集训练得到。我们使用 来得到关于样本的梯度估计，并使用该梯度来训练基学习器并得到最终的模型。 Ordered boosting算法好是好，但是在大部分的实际任务当中都不具备使用价值，因为需要训练 个不同的模型，大大增加的内存消耗和时间复杂度。在CatBoost当中，我们以决策树为基学习器的梯度提升算法的基础上，对该算法进行了改进。 前面提到过，在传统的GBDT框架当中，构建下一棵树分为两个阶段：选择树结构和在树结构固定后计算叶子节点的值。CatBoost主要在第一阶段进行优化。在建树的阶段，CatBoost有两种提升模式，Ordered和Plain。Plain模式是采用内建的ordered TS对类别型特征进行转化后的标准GBDT算法。Ordered则是对Ordered boosting算法的优化。两种提升模式的具体介绍可以翻看论文《CatBoost: unbiased boosting with categorical features》。 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:14:2","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"快速评分 CatBoost使用对称树（oblivious trees）作为基预测器。在这类树中，相同的分割准则在树的整个一层上使用。这种树是平衡的，不太容易过拟合。梯度提升对称树被成功地用于各种学习任务中。在对称树中，每个叶子节点的索引可以被编码为长度等于树深度的二进制向量。这在CatBoost模型评估器中得到了广泛的应用：我们首先将所有浮点特征、统计信息和独热编码特征进行二值化，然后使用二进制特征来计算模型预测值。 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:15:0","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"基于GPU实现快速训练 密集的数值特征。 对于任何GBDT算法而言，最大的难点之一就是搜索最佳分割。尤其是对于密集的数值特征数据集来说，该步骤是建立决策树时的主要计算负担。CatBoost使用oblivious 决策树作为基模型，并将特征离散化到固定数量的箱子中以减少内存使用。就GPU内存使用而言，CatBoost至少与LightGBM一样有效。主要改进之处就是利用了一种不依赖于原子操作的直方图计算方法。 类别型特征。 CatBoost实现了多种处理类别型特征的方法，并使用完美哈希来存储类别型特征的值，以减少内存使用。由于GPU内存的限制，在CPU RAM中存储按位压缩的完美哈希，以及要求的数据流、重叠计算和内存等操作。通过哈希来分组观察。在每个组中，我们需要计算一些统计量的前缀和。该统计量的计算使用分段扫描GPU图元实现。 多GPU支持。 CatBoost中的GPU实现可支持多个GPU。分布式树学习可以通过数据或特征进行并行化。CatBoost采用多个学习数据集排列的计算方案，在训练期间计算类别型特征的统计数据。 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:16:0","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"CatBoost的优缺点 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:17:0","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"优点 性能卓越： 在性能方面可以匹敌任何先进的机器学习算法； 鲁棒性/强健性： 它减少了对很多超参数调优的需求，并降低了过度拟合的机会，这也使得模型变得更加具有通用性； 易于使用： 提供与scikit集成的Python接口，以及R和命令行界面； 实用： 可以处理类别型、数值型特征； 可扩展： 支持自定义损失函数； ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:17:1","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"缺点 对于类别型特征的处理需要大量的内存和时间； 不同随机数的设定对于模型预测结果有一定的影响； ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:17:2","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"实例 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:18:0","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"数据集 这里我使用了 2015 年航班延误的 Kaggle 数据集，其中同时包含类别型变量和数值变量。这个数据集中一共有约 500 万条记录，我使用了 1% 的数据：5 万行记录。数据集官方地址：https://www.kaggle.com/usdot/flight-delays#flights.csv 。以下是建模使用的特征： 月、日、星期： 整型数据 航线或航班号： 整型数据 出发、到达机场： 数值数据 出发时间： 浮点数据 距离和飞行时间： 浮点数据 到达延误情况： 这个特征作为预测目标，并转为二值变量：航班是否延误超过 10 分钟 实验说明： 在对 CatBoost 调参时，很难对类别型特征赋予指标。因此，同时给出了不传递类别型特征时的调参结果，并评估了两个模型：一个包含类别型特征，另一个不包含。如果未在cat_features参数中传递任何内容，CatBoost会将所有列视为数值变量。注意，如果某一列数据中包含字符串值，CatBoost 算法就会抛出错误。另外，带有默认值的 int 型变量也会默认被当成数值数据处理。在 CatBoost 中，必须对变量进行声明，才可以让算法将其作为类别型变量处理。 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:18:1","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"不加Categorical features选项的代码 import pandas as pd, numpy as np from sklearn.model_selection import train_test_split, GridSearchCV from sklearn import metrics import catboost as cb # 一共有约 500 万条记录，我使用了 1% 的数据：5 万行记录 # data = pd.read_csv(\"flight-delays/flights.csv\") # data = data.sample(frac=0.1, random_state=10) # 500-\u003e50 # data = data.sample(frac=0.1, random_state=10) # 50-\u003e5 # data.to_csv(\"flight-delays/min_flights.csv\") # 读取 5 万行记录 data = pd.read_csv(\"flight-delays/min_flights.csv\") print(data.shape) # (58191, 31) data = data[[\"MONTH\", \"DAY\", \"DAY_OF_WEEK\", \"AIRLINE\", \"FLIGHT_NUMBER\", \"DESTINATION_AIRPORT\", \"ORIGIN_AIRPORT\", \"AIR_TIME\", \"DEPARTURE_TIME\", \"DISTANCE\", \"ARRIVAL_DELAY\"]] data.dropna(inplace=True) data[\"ARRIVAL_DELAY\"] = (data[\"ARRIVAL_DELAY\"] \u003e 10) * 1 cols = [\"AIRLINE\", \"FLIGHT_NUMBER\", \"DESTINATION_AIRPORT\", \"ORIGIN_AIRPORT\"] for item in cols: data[item] = data[item].astype(\"category\").cat.codes + 1 train, test, y_train, y_test = train_test_split(data.drop([\"ARRIVAL_DELAY\"], axis=1), data[\"ARRIVAL_DELAY\"], random_state=10, test_size=0.25) cat_features_index = [0, 1, 2, 3, 4, 5, 6] def auc(m, train, test): return (metrics.roc_auc_score(y_train, m.predict_proba(train)[:, 1]), metrics.roc_auc_score(y_test, m.predict_proba(test)[:, 1])) # 调参，用网格搜索调出最优参数 params = {'depth': [4, 7, 10], 'learning_rate': [0.03, 0.1, 0.15], 'l2_leaf_reg': [1, 4, 9], 'iterations': [300, 500]} cb = cb.CatBoostClassifier() cb_model = GridSearchCV(cb, params, scoring=\"roc_auc\", cv=3) cb_model.fit(train, y_train) # 查看最佳分数 print(cb_model.best_score_) # 0.7088001891107445 # 查看最佳参数 print(cb_model.best_params_) # {'depth': 4, 'iterations': 500, 'l2_leaf_reg': 9, 'learning_rate': 0.15} # With Categorical features，用最优参数拟合数据 clf = cb.CatBoostClassifier(eval_metric=\"AUC\", depth=4, iterations=500, l2_leaf_reg=9, learning_rate=0.15) clf.fit(train, y_train) print(auc(clf, train, test)) # (0.7809684655761157, 0.7104617034553192) ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:18:2","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"有Categorical features选项的代码 import pandas as pd, numpy as np from sklearn.model_selection import train_test_split, GridSearchCV from sklearn import metrics import catboost as cb # 读取 5 万行记录 data = pd.read_csv(\"flight-delays/min_flights.csv\") print(data.shape) # (58191, 31) data = data[[\"MONTH\", \"DAY\", \"DAY_OF_WEEK\", \"AIRLINE\", \"FLIGHT_NUMBER\", \"DESTINATION_AIRPORT\", \"ORIGIN_AIRPORT\", \"AIR_TIME\", \"DEPARTURE_TIME\", \"DISTANCE\", \"ARRIVAL_DELAY\"]] data.dropna(inplace=True) data[\"ARRIVAL_DELAY\"] = (data[\"ARRIVAL_DELAY\"] \u003e 10) * 1 cols = [\"AIRLINE\", \"FLIGHT_NUMBER\", \"DESTINATION_AIRPORT\", \"ORIGIN_AIRPORT\"] for item in cols: data[item] = data[item].astype(\"category\").cat.codes + 1 train, test, y_train, y_test = train_test_split(data.drop([\"ARRIVAL_DELAY\"], axis=1), data[\"ARRIVAL_DELAY\"], random_state=10, test_size=0.25) cat_features_index = [0, 1, 2, 3, 4, 5, 6] def auc(m, train, test): return (metrics.roc_auc_score(y_train, m.predict_proba(train)[:, 1]), metrics.roc_auc_score(y_test, m.predict_proba(test)[:, 1])) # With Categorical features clf = cb.CatBoostClassifier(eval_metric=\"AUC\", one_hot_max_size=31, depth=4, iterations=500, l2_leaf_reg=9, learning_rate=0.15) clf.fit(train, y_train, cat_features=cat_features_index) print(auc(clf, train, test)) # (0.7817912095285117, 0.7152541135019913) ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:18:3","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["机器学习"],"content":"CatBoost与XGBoost、LightGBM的联系与区别 （1）2014年3月XGBoost算法首次被陈天奇提出，但是直到2016年才逐渐著名。2017年1月微软发布LightGBM第一个稳定版本。2017年4月Yandex开源CatBoost。自从XGBoost被提出之后，很多文章都在对其进行各种改进，CatBoost和LightGBM就是其中的两种。 （2）CatBoost处理类别型特征十分灵活，可直接传入类别型特征的列标识，模型会自动将其使用One-hot编码，还可通过设置 one_hot_max_size参数来限制One-hot特征向量的长度。如果不传入类别型特征的列标识，那么CatBoost会把所有列视为数值特征。对于One-hot编码超过设定的one_hot_max_size值的特征来说，CatBoost将会使用一种高效的encoding方法，与mean encoding类似，但是会降低过拟合。处理过程如下： 将输入样本集随机排序，并生成多组随机排列的情况； 将浮点型或属性值标记转化为整数； 将所有的类别型特征值结果都根据以下公式，转化为数值结果； $$ avg_target = \\frac {countInClass + prior}{totalCount + 1} $$ 其中 countInClass 表示在当前类别型特征值中有多少样本的标记值是；prior 是分子的初始值，根据初始参数确定。totalCount 是在所有样本中（包含当前样本）和当前样本具有相同的类别型特征值的样本数量。 LighGBM 和 CatBoost 类似，也可以通过使用特征名称的输入来处理类别型特征数据，它没有对数据进行独热编码，因此速度比独热编码快得多。LighGBM 使用了一个特殊的算法来确定属性特征的分割值。 train_data = lgb.Dataset(data, label=label, feature_name=['c1', 'c2', 'c3'], categorical_feature=['c3']) 注意，在建立适用于 LighGBM 的数据集之前，需要将类别型特征变量转化为整型变量，此算法不允许将字符串数据传给类别型变量参数。 XGBoost 和 CatBoost、 LighGBM 算法不同，XGBoost 本身无法处理类别型特征，而是像随机森林一样，只接受数值数据。因此在将类别型特征数据传入 XGBoost 之前，必须通过各种编码方式：例如，序号编码、独热编码和二进制编码等对数据进行处理。 集成学习 ","date":"2020-02-10","objectID":"https://mayuanucas.github.io/xgboost-lightgbm/:19:0","tags":["机器学习","数据挖掘","sklearn"],"title":"XGBoost、LightGBM、CatBoost","uri":"https://mayuanucas.github.io/xgboost-lightgbm/"},{"categories":["效率工具"],"content":"本文对 GitHub 的使用进行介绍. 前言 GitHub是通过Git进行版本控制的软件源代码托管服务平台，于2008年4月上线，2018年6月被微软公司收购。GitHub同时提供付费账户和免费账户，这两种账户都可以创建公开或私有的代码仓库，但付费用户支持更多功能。除了允许个人或组织创建和访问保管中的代码以外，GitHub还提供了一些方便共同开发软件的功能，例如：允许用户追踪其他用户、组织、软件库的动态，对软件代码的改动或bug提出评论等。GitHub还提供了图表功能，用于显示开发者在代码库上工作以及软件项目的开发活跃程度。 GitHub 2019年度报告显示，在过去的一年中，GitHub新增了一千万用户，现在总共有超过四千万用户，GitHub上的仓库数量超过 1 亿。 Git与GitHub的区别和联系 谈到 GitHub, 就必定会提到Git。 Git是一个开源的分布式版本控制系统,简单来说，Git 是一个管理\"代码的历史记录\"的工具,而 GitHub 本质上是一个代码托管平台，它提供的是基于 Git 的代码托管服务。对于一个团队来说，即使不使用 GitHub，也可以通过自己搭建和管理 Git 服务器来进行代码库的管理，或者选择其它一些代码托管平台，如 Gitee(码云), GitLab等。 使用 Git 管理代码前需要从 Git 官网 https://git-scm.com/ 下载相应平台的安装包并完成安装. Git 的本身不具备图形界面，一般只能在终端输入命令进行使用.但是在安装 Git 的同时，其实也装好了它提供的可视化工具，gitk 和 git-gui. gitk 是一个历史记录的图形化查看器, git-gui 主要是一个用来制作提交的工具. GitHub 也发布了面向工作流程的 Git 客户端：提供了Windows 版和 Mac 版,它们很好的展示了一个面向工作流程的工具——专注于提升常用的功能及提高协作效率. GitHub概览 ","date":"2020-01-04","objectID":"https://mayuanucas.github.io/github%E6%8E%A2%E7%B4%A2/:0:0","tags":["GitHub"],"title":"Github入门与实践","uri":"https://mayuanucas.github.io/github%E6%8E%A2%E7%B4%A2/"},{"categories":["效率工具"],"content":"基础应用场景 GitHub 的基础应用场景是作为远程的代码存储,代码版本控制. ","date":"2020-01-04","objectID":"https://mayuanucas.github.io/github%E6%8E%A2%E7%B4%A2/:1:0","tags":["GitHub"],"title":"Github入门与实践","uri":"https://mayuanucas.github.io/github%E6%8E%A2%E7%B4%A2/"},{"categories":["效率工具"],"content":"常用应用场景 ","date":"2020-01-04","objectID":"https://mayuanucas.github.io/github%E6%8E%A2%E7%B4%A2/:2:0","tags":["GitHub"],"title":"Github入门与实践","uri":"https://mayuanucas.github.io/github%E6%8E%A2%E7%B4%A2/"},{"categories":["效率工具"],"content":"协同开发 ","date":"2020-01-04","objectID":"https://mayuanucas.github.io/github%E6%8E%A2%E7%B4%A2/:2:1","tags":["GitHub"],"title":"Github入门与实践","uri":"https://mayuanucas.github.io/github%E6%8E%A2%E7%B4%A2/"},{"categories":["效率工具"],"content":"获取(学习)优秀的开源项目 由于存放在 Github 公有仓库的代码是公开的，所以可以很方便的获取、使用、学习这些优秀开源项目的代码和文档. 国内外科技公司 国外 GitHub地址 国内 GitHub地址 Google https://github.com/google 阿里巴巴 https://github.com/alibaba Facebook https://github.com/facebook 腾讯 https://github.com/Tencent Microsoft https://github.com/microsoft 滴滴 https://github.com/didi 世界闻名的技术专家 Linux 发明者 Linus Torvalds：https://github.com/torvalds Hands-On Machine Learning with Scikit-Learn and TensorFlow 的作者 Aurélien Geron：https://github.com/ageron 优秀的开源项目 项目 GitHub地址 TensorFlow https://github.com/tensorflow/tensorflow scikit-learn https://github.com/scikit-learn/scikit-learn pytorch https://github.com/pytorch/pytorch ","date":"2020-01-04","objectID":"https://mayuanucas.github.io/github%E6%8E%A2%E7%B4%A2/:2:2","tags":["GitHub"],"title":"Github入门与实践","uri":"https://mayuanucas.github.io/github%E6%8E%A2%E7%B4%A2/"},{"categories":["效率工具"],"content":"其它应用场景 ","date":"2020-01-04","objectID":"https://mayuanucas.github.io/github%E6%8E%A2%E7%B4%A2/:3:0","tags":["GitHub"],"title":"Github入门与实践","uri":"https://mayuanucas.github.io/github%E6%8E%A2%E7%B4%A2/"},{"categories":["效率工具"],"content":"搭建个人网站 基于 GitHub Pages 搭建博客，不仅搭建简单，同时还可自定义样式、绑定域名。 ","date":"2020-01-04","objectID":"https://mayuanucas.github.io/github%E6%8E%A2%E7%B4%A2/:3:1","tags":["GitHub"],"title":"Github入门与实践","uri":"https://mayuanucas.github.io/github%E6%8E%A2%E7%B4%A2/"},{"categories":["效率工具"],"content":"接触优秀开发者的有效渠道 GitHub 个人主页会有联系邮箱、个人网站等信息，通过这些信息可以与技术专家进行沟通交流。 ","date":"2020-01-04","objectID":"https://mayuanucas.github.io/github%E6%8E%A2%E7%B4%A2/:3:2","tags":["GitHub"],"title":"Github入门与实践","uri":"https://mayuanucas.github.io/github%E6%8E%A2%E7%B4%A2/"},{"categories":["效率工具"],"content":"用GitHub协作翻译 国内的一些社区会用 GitHub 组织志愿者进行文档的协作翻译。 个人仪表板 个人仪表板是登录 GitHub 时显示的第一页。访问个人仪表板，可以跟踪参与或关注的议题和拉取请求，浏览常用仓库和团队页面，了解订阅的组织和仓库中近期活动的最新信息，以及探索推荐的仓库。登录后要访问个人仪表板，在任意页面点击左上角的网站图标就能跳转到仪表板页面。 个人资料(Profile) 个人资料页面不仅展示了开发者的个人介绍, 联系邮箱, 博客地址, 社交账号, 还展示了开发者创建的或 Fork 的仓库, 页面最下方还展示了开发者每天的活跃程度(每天提交的 commit 越多, 对应日期的小方格颜色越深). 仓库 GitHub 常用术语介绍: Repository：简称Repo，可以理解为“仓库”，我们的项目就存放在仓库之中。也就是说，如果我们想要建立项目，就得先建立仓库；有多个项目，就建立多个仓库。 Issues：可以理解为“问题”，举一个简单的例子，如果我们开源一个项目，如果别人看了我们的项目，并且发现了bug，或者感觉那个地方有待改进，他就可以给我们提出Issue，等我们把Issues解决之后，就可以把这些Issues关闭；反之，我们也可以给他人提出Issue。 Watch：可以理解为“观察”，如果我们Watch了一个项目，之后，如果这个项目有了任何更新，我们都会在第一时候收到该项目的更新通知。 Star：可以理解为“点赞”，当我们感觉某一个项目做的比较好之后，就可以为这个项目点赞，而且我们点赞过的项目，都会保存到我们的Star之中，方便我们随时查看。在 GitHub 之中，如果一个项目的点星数能够超百，那么说明这个项目已经很不错了。 Fork：可以理解为“拉分支”，如果我们对某一个项目比较感兴趣，并且想在此基础之上开发新的功能，这时我们就可以Fork这个项目，这表示复制一个完成相同的项目到我们的 GitHub 账号之中，而且独立于原项目。之后，我们就可以在自己复制的项目中进行开发了。 Pull Request：可以理解为“提交请求”，此功能是建立在Fork之上的，如果我们Fork了一个项目，对其进行了修改，而且感觉修改的还不错，我们就可以对原项目的拥有者提出一个Pull请求，等其对我们的请求审核，并且通过审核之后，就可以把我们修改过的内容合并到原项目之中，这时我们就成了该项目的贡献者。 Merge：可以理解为“合并”，如果别人Fork了我们的项目，对其进行了修改，并且提出了Pull请求，这时我们就可以对这个Pull请求进行审核。如果这个Pull请求的内容满足我们的要求，并且跟我们原有的项目没有冲突的话，就可以将其合并到我们的项目之中。当然，是否进行合并，由我们决定。 Gist：如果我们没有项目可以开源或者只是单纯的想分享一些代码片段的话，我们就可以选择Gist。 Topics GitHub Topic 页面展示了最新和最流行的讨论主题，在这里不仅能够看到开发项目，还能看到很多非开发技术的讨论主题. Trending GitHub Trending 页面展示了每天/每周/每月周期的热门 Repositories 和 Developers，可以看到在某个周期处于热门状态的开发项目和开发者。 GitHub搜索 ","date":"2020-01-04","objectID":"https://mayuanucas.github.io/github%E6%8E%A2%E7%B4%A2/:3:3","tags":["GitHub"],"title":"Github入门与实践","uri":"https://mayuanucas.github.io/github%E6%8E%A2%E7%B4%A2/"},{"categories":["效率工具"],"content":"搜索开发者 比如需要寻找国产软件，搜索时设置 location 为 china，如果要寻找使用 javascript 语言开发者，则再增加 language 为 javascript，整个搜索条件就是：language:javascript location:china，从搜索结果来看，我们找到了超过 2.1 万名地区信息填写为 china 的 javascript 开发者，朋友们熟悉的阮一峰老师排在前列。根据官方手册，搜索 GitHub 用户时还支持使用 followers、in:fullname 组合条件进行搜索。 ","date":"2020-01-04","objectID":"https://mayuanucas.github.io/github%E6%8E%A2%E7%B4%A2/:4:0","tags":["GitHub"],"title":"Github入门与实践","uri":"https://mayuanucas.github.io/github%E6%8E%A2%E7%B4%A2/"},{"categories":["效率工具"],"content":"搜索仓库 在 GitHub 上找到优秀的项目和工具，通过关键字或者设置搜索条件能够帮助我们事半功倍找到好资源。 ","date":"2020-01-04","objectID":"https://mayuanucas.github.io/github%E6%8E%A2%E7%B4%A2/:5:0","tags":["GitHub"],"title":"Github入门与实践","uri":"https://mayuanucas.github.io/github%E6%8E%A2%E7%B4%A2/"},{"categories":["效率工具"],"content":"设定搜索条件 如果明确需要寻找某类特定的项目，比如用某种语言开发、Stars 数量需要达到标准的项目，在搜索框中直接输入搜索条件即可。其中用于发现项目，常用的搜索条件有：stars:、language:、forks:、in:，这些条件是设置搜索条件为项目收藏数量、开发语言、Fork数量. 比如输入 stars:\u003e=5000 language:python，得到的结果 就是收藏大于和等于 5000 的 python 项目。 通过 in: 限定符，可以将搜索限制为仓库名称、仓库说明、自述文件内容或这些的任意组合。 限定符 示例 in:name python in:name 匹配其名称中含有 “python” 的仓库。 in:description python in:name,description 匹配其名称或说明中含有 “python” 的仓库。 in:readme python in:readme 匹配其自述文件中提及 “python” 的仓库。 如果觉得记住这些搜索条件略显繁琐的话，使用 GitHub 提供的高级搜索功能同样可自定义条件进行搜索。或者参考官方给出的帮助指南 Searching on GitHub ，里面有更多关于项目、代码、评论、问题等搜索技巧。 结语 GitHub 网站上有很多优秀的开源项目，利用 GitHub 提供的各种功能，包括高级搜索、Topic、Trending 等专题页面，不仅可以帮助我们发现更多好用的效率工具和开源项目, 而且还能帮助我们了解业界最新的研究动态, 提高开发能力。 ","date":"2020-01-04","objectID":"https://mayuanucas.github.io/github%E6%8E%A2%E7%B4%A2/:5:1","tags":["GitHub"],"title":"Github入门与实践","uri":"https://mayuanucas.github.io/github%E6%8E%A2%E7%B4%A2/"}]